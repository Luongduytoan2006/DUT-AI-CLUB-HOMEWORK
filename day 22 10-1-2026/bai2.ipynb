{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bc1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0497874e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8556, -1.3365],\n",
       "         [-0.9221, -1.0629],\n",
       "         [-0.2027, -0.8219],\n",
       "         ...,\n",
       "         [ 1.5530,  0.2719],\n",
       "         [ 0.3596, -0.9259],\n",
       "         [ 0.2630,  1.5602]]),\n",
       " tensor([2, 2, 2, 1, 2, 2, 1, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 1, 0, 0, 2, 1,\n",
       "         2, 0, 2, 1, 2, 1, 0, 1, 2, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "         0, 0, 1, 2, 1, 0, 0, 2, 2, 0, 0, 0, 0, 1, 2, 1, 0, 2, 1, 1, 1, 2, 2, 0,\n",
       "         2, 2, 0, 1, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 2, 2, 2, 0, 0, 1, 2, 0, 0, 0,\n",
       "         1, 0, 1, 2, 1, 0, 0, 1, 1, 2, 1, 2, 2, 2, 0, 2, 2, 1, 0, 0, 0, 1, 0, 1,\n",
       "         1, 0, 1, 2, 0, 0, 1, 2, 2, 1, 0, 1, 0, 0, 2, 1, 1, 1, 1, 0, 1, 1, 2, 2,\n",
       "         1, 0, 2, 2, 0, 2, 0, 1, 1, 0, 0, 1, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 2,\n",
       "         1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 2, 1, 1, 2, 1, 2, 0, 1, 1, 2, 2, 0, 0,\n",
       "         2, 2, 0, 0, 2, 2, 2, 2, 0, 1, 2, 0, 0, 2, 1, 1, 1, 1, 1, 0, 2, 0, 2, 2,\n",
       "         0, 1, 0, 1, 2, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 0, 0, 1, 0, 2, 0, 0, 2, 0,\n",
       "         1, 1, 2, 2, 1, 2, 0, 0, 1, 0, 0, 1, 1, 0, 1, 2, 1, 1, 0, 1, 0, 0, 1, 2,\n",
       "         2, 1, 2, 1, 0, 0, 2, 0, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 0, 1, 0, 1,\n",
       "         0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 2, 2, 2, 2, 0, 1, 2, 0, 2, 1,\n",
       "         2, 0, 1, 2, 1, 1, 2, 2, 1, 1, 0, 2, 0, 2, 1, 2, 0, 0, 0, 2, 2, 2, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 0,\n",
       "         0, 1, 0, 2, 0, 1, 1, 2, 1, 2, 2, 0, 0, 2, 0, 1, 2, 0, 0, 2, 0, 1, 0, 2,\n",
       "         2, 0, 2, 1, 0, 2, 1, 2, 0, 0, 1, 2, 2, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 2, 2, 2, 2, 0, 1, 0, 2, 1, 2, 0, 2, 1, 0, 2, 1, 2, 0,\n",
       "         1, 1, 2, 0, 0, 1, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 0,\n",
       "         1, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2,\n",
       "         0, 2, 0, 0, 1, 0, 0, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0,\n",
       "         1, 0, 1, 0, 1, 2, 0, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2,\n",
       "         2, 1, 2, 1, 0, 0, 2, 2, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 2, 1, 2, 2, 0,\n",
       "         2, 0, 2, 0, 1, 0, 1, 1, 2, 2, 1, 1, 0, 0, 0, 2, 0, 0, 1, 2, 0, 1, 0, 2,\n",
       "         1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 2, 2, 1, 1, 0, 2, 0, 1, 1, 2, 1, 1, 0, 0,\n",
       "         2, 2, 2, 0, 0, 1, 2, 1, 1, 2, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0,\n",
       "         0, 1, 1, 1, 1, 2, 1, 2, 1, 2, 0, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 0,\n",
       "         0, 2, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 2, 2, 2, 1, 0, 2, 2, 2, 0, 0, 1, 2,\n",
       "         1, 2, 1, 0, 2, 0, 2, 0, 1, 0, 0, 0, 2, 2, 0, 2, 1, 1, 2, 1, 2, 1, 2, 1,\n",
       "         1, 0, 0, 1, 0, 2, 0, 2, 2, 2, 1, 1, 0, 2, 2, 1, 2, 2, 2, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0, 1, 2, 0, 1, 1, 2, 2, 1, 2, 2, 0, 1, 1,\n",
       "         2, 1, 1, 2, 1, 0, 2, 2, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0,\n",
       "         2, 2, 2, 1, 0, 2, 0, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 1, 0, 0, 2, 1, 2, 0,\n",
       "         2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 0, 2, 0, 0, 0, 0,\n",
       "         0, 0, 1, 2, 2, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 1, 1,\n",
       "         1, 1, 1, 2, 0, 2, 0, 1, 1, 2, 1, 1, 1, 1, 0, 2, 1, 0, 0, 2, 1, 2, 0, 0,\n",
       "         1, 2, 2, 0, 0, 2, 1, 0, 1, 2, 2, 2, 0, 1, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0,\n",
       "         1, 1, 2, 1, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 0, 0, 2, 0, 2, 2, 0,\n",
       "         1, 2, 2, 0, 0, 2, 2, 1, 2, 1, 1, 0, 0, 0, 2, 2, 0, 2, 2, 1, 1, 2, 2, 2,\n",
       "         0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 2, 2, 0, 1, 0, 0, 1, 0, 2, 0, 1,\n",
       "         1, 0, 2, 2, 2, 1, 2, 0, 1, 0, 2, 2, 1, 2, 0, 0, 0, 2, 0, 1, 0, 0, 1, 2,\n",
       "         1, 0, 2, 2, 0, 2, 2, 0, 1, 0, 1, 0, 2, 1, 2, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create synthetic nonlinear data since NonLinear_data.npy doesn't exist\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, labels = make_classification(\n",
    "    n_samples=1000, \n",
    "    n_features=2, \n",
    "    n_informative=2, \n",
    "    n_redundant=0, \n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f7d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.855597</td>\n",
       "      <td>-1.336502</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.922102</td>\n",
       "      <td>-1.062900</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.202714</td>\n",
       "      <td>-0.821910</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.969746</td>\n",
       "      <td>1.537000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.008583</td>\n",
       "      <td>-0.796954</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_0  Feature_1  Target\n",
       "0  -0.855597  -1.336502       2\n",
       "1  -0.922102  -1.062900       2\n",
       "2  -0.202714  -0.821910       2\n",
       "3   0.969746   1.537000       1\n",
       "4  -1.008583  -0.796954       2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np = X.detach().cpu().numpy()\n",
    "\n",
    "column_names = [f\"Feature_{i}\" for i in range(X_np.shape[1])]\n",
    "\n",
    "df = pd.DataFrame(X_np, columns=column_names)\n",
    "\n",
    "df['Target'] = labels.detach().cpu().numpy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052dbc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So dac trung: 2\n",
      "So nhan: 3\n",
      "Phan phoi nhan: [332 333 335]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.drop(['Target'], axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "print('So dac trung:', x.shape[1])\n",
    "print('So nhan:', len(y.unique()))\n",
    "print('Phan phoi nhan:', np.bincount(y))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8e4f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 700 samples\n",
      "  Class distribution: [232 233 235]\n",
      "Val set: 150 samples\n",
      "  Class distribution: [50 50 50]\n",
      "Test set: 150 samples\n",
      "  Class distribution: [50 50 50]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {x_train.shape[0]} samples\")\n",
    "print(f\"  Class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Val set: {x_val.shape[0]} samples\")\n",
    "print(f\"  Class distribution: {np.bincount(y_val)}\")\n",
    "print(f\"Test set: {x_test.shape[0]} samples\")\n",
    "print(f\"  Class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "279e2543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalized using StandardScaler\n",
      "Train mean: [9.0258458e-09 1.5837806e-08]... (first 3 features)\n",
      "Train std: [0.9999995 1.       ]... (first 3 features)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "print(\"Features normalized using StandardScaler\")\n",
    "print(f\"Train mean: {x_train_scaled.mean(axis=0)[:3]}... (first 3 features)\")\n",
    "print(f\"Train std: {x_train_scaled.std(axis=0)[:3]}... (first 3 features)\")\n",
    "\n",
    "x_train, x_val, x_test = x_train_scaled, x_val_scaled, x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfcd9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 2)\n",
      "Output shape: (700, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.83346102,  0.76890649,  0.97184944, ..., -1.62737992,\n",
       "        -0.28112454,  1.05088935],\n",
       "       [-0.28729437,  0.31496415, -0.37250229, ..., -0.25990366,\n",
       "         0.5390563 ,  0.33913334],\n",
       "       [-0.70938085,  0.66670393,  0.65331473, ..., -1.3111288 ,\n",
       "        -0.08299955,  0.88876144],\n",
       "       ...,\n",
       "       [-0.7447876 ,  0.75400271, -0.07968731, ..., -1.05078775,\n",
       "         0.60105306,  0.90811474],\n",
       "       [-1.25885309,  1.2170133 ,  0.6790105 , ..., -2.12230508,\n",
       "         0.28448871,  1.56148892],\n",
       "       [-0.7822455 ,  0.74419187,  0.59277339, ..., -1.39148612,\n",
       "         0.02321509,  0.97588204]], shape=(700, 100))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        self.weights = np.random.uniform(-1, 1, (input_dim, output_dim))\n",
    "        self.biases = np.zeros((output_dim,))\n",
    "\n",
    "        self.input = None\n",
    "\n",
    "        self.num_params = output_dim * input_dim + output_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return x @ self.weights + self.biases\n",
    "    \n",
    "    def backward(self, grad_output, lr):\n",
    "        grad_input = grad_output @ self.weights.T\n",
    "\n",
    "        self.lr = lr\n",
    "        m = self.input.shape[0]\n",
    "\n",
    "        d_weights = (self.input.T @ grad_output) / m\n",
    "        d_biases = np.mean(grad_output, axis=0)\n",
    "\n",
    "        self.weights -= lr * d_weights\n",
    "        self.biases -= lr * d_biases\n",
    "\n",
    "        return grad_input\n",
    "    \n",
    "linear = Linear(2, 100)\n",
    "print(x_train.shape)\n",
    "output = linear.forward(x_train)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18e74173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (700, 100)\n",
      "[[0.         0.76890649 0.97184944 ... 0.         0.         1.05088935]\n",
      " [0.         0.31496415 0.         ... 0.         0.5390563  0.33913334]\n",
      " [0.         0.66670393 0.65331473 ... 0.         0.         0.88876144]\n",
      " ...\n",
      " [0.         0.75400271 0.         ... 0.         0.60105306 0.90811474]\n",
      " [0.         1.2170133  0.6790105  ... 0.         0.28448871 1.56148892]\n",
      " [0.         0.74419187 0.59277339 ... 0.         0.02321509 0.97588204]]\n"
     ]
    }
   ],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output * (self.input > 0).astype(float)\n",
    "        return grad_input\n",
    "    \n",
    "relu = ReLU()\n",
    "output = relu.forward(output)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "630bd641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (700, 100)\n"
     ]
    }
   ],
   "source": [
    "class Softmax():\n",
    "    def forward(self, x):\n",
    "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, y, y_pred):\n",
    "        m = y.shape[0]\n",
    "        d_scores = y_pred.copy()\n",
    "        d_scores[np.arange(m), y] -= 1\n",
    "        d_scores /= m\n",
    "        return d_scores\n",
    "\n",
    "softmax = Softmax()\n",
    "output = softmax.forward(output)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27e004ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, y_pred):\n",
    "    n = y.shape[0]\n",
    "    n_classes = y_pred.shape[1]\n",
    "    y_onehot = np.zeros((n, n_classes))\n",
    "    y_onehot[np.arange(n), y] = 1\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = -np.mean(np.sum(y_onehot * np.log(y_pred), axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752596cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear1 parameters: 300\n",
      "Linear2 parameters: 20200\n",
      "Linear3 parameters: 20100\n",
      "Linear4 parameters: 303\n",
      "total parameters: 40903\n",
      "Output shape: (700, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.99999991e-001, 9.04556447e-009, 6.29004551e-147],\n",
       "       [9.50407921e-011, 1.00000000e+000, 9.36056032e-034],\n",
       "       [9.99999999e-001, 7.43293267e-010, 2.42279758e-112],\n",
       "       ...,\n",
       "       [1.77206047e-002, 9.82279395e-001, 1.43584942e-073],\n",
       "       [1.00000000e+000, 1.33662742e-018, 1.05066196e-160],\n",
       "       [1.00000000e+000, 2.05233611e-012, 4.61091510e-114]],\n",
       "      shape=(700, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        self.linear1 = Linear(input_dim, 100)\n",
    "        print(f\"Linear1 parameters: {self.linear1.num_params}\")\n",
    "        self.relu1 = ReLU()\n",
    "\n",
    "        self.linear2 = Linear(100, 200)\n",
    "        print(f\"Linear2 parameters: {self.linear2.num_params}\")\n",
    "        self.relu2 = ReLU()\n",
    "\n",
    "        self.linear3 = Linear(200, 100)\n",
    "        print(f\"Linear3 parameters: {self.linear3.num_params}\")\n",
    "        self.relu3 = ReLU()\n",
    "\n",
    "        self.linear4 = Linear(100, output_dim)\n",
    "        print(f\"Linear4 parameters: {self.linear4.num_params}\")\n",
    "        self.softmax = Softmax()\n",
    "        print(f\"total parameters: {self.linear1.num_params + self.linear2.num_params + self.linear3.num_params + self.linear4.num_params}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1.forward(x)\n",
    "        out = self.relu1.forward(out)\n",
    "        out = self.linear2.forward(out)\n",
    "        out = self.relu2.forward(out)\n",
    "        out = self.linear3.forward(out)\n",
    "        out = self.relu3.forward(out)\n",
    "        out = self.linear4.forward(out)\n",
    "        out = self.softmax.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, y, y_pred, lr):\n",
    "        grad_softmax = softmax.backward(y, y_pred)\n",
    "        grad = self.linear4.backward(grad_softmax, lr)\n",
    "        grad = self.relu3.backward(grad)\n",
    "        grad = self.linear3.backward(grad, lr)\n",
    "        grad = self.relu2.backward(grad)\n",
    "        grad = self.linear2.backward(grad, lr)\n",
    "        grad = self.relu1.backward(grad)\n",
    "        grad = self.linear1.backward(grad, lr)\n",
    "        return grad\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        np.savez(\n",
    "            filepath,\n",
    "            linear1_weights=self.linear1.weights,\n",
    "            linear1_biases=self.linear1.biases,\n",
    "            linear2_weights=self.linear2.weights,\n",
    "            linear2_biases=self.linear2.biases,\n",
    "            linear3_weights=self.linear3.weights,\n",
    "            linear3_biases=self.linear3.biases,\n",
    "            linear4_weights=self.linear4.weights,\n",
    "            linear4_biases=self.linear4.biases,\n",
    "        )\n",
    "\n",
    "    def load(self, filepath):\n",
    "        data = np.load(filepath)\n",
    "        self.linear1.weights = data['linear1_weights']\n",
    "        self.linear1.biases = data['linear1_biases']\n",
    "        self.linear2.weights = data['linear2_weights']\n",
    "        self.linear2.biases = data['linear2_biases']\n",
    "        self.linear3.weights = data['linear3_weights']\n",
    "        self.linear3.biases = data['linear3_biases']\n",
    "        self.linear4.weights = data['linear4_weights']\n",
    "        self.linear4.biases = data['linear4_biases']\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "    \n",
    "    def fit(self, x_train, y_train, x_val, y_val, epochs: int, lr: float):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(x_train)\n",
    "            self.backward(y_train, y_pred, lr)\n",
    "\n",
    "            train_loss = cross_entropy(y_train, y_pred)\n",
    "            train_losses.append(train_loss)\n",
    "            train_acc = np.mean(np.argmax(y_pred, axis=1) == y_train)\n",
    "            self.save(\"last-nonlinear-model.npz\")\n",
    "\n",
    "            y_pred_val = self.forward(x_val)\n",
    "            val_loss = cross_entropy(y_val, y_pred_val)\n",
    "            if val_losses and val_loss < val_losses[-1]:\n",
    "                self.save(\"best-nonlinear-model.npz\")\n",
    "            val_losses.append(val_loss)\n",
    "            val_acc = np.mean(np.argmax(y_pred_val, axis=1) == y_val)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = self.forward(x)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "nn = ArtificialNeuralNetwork(2, 3)\n",
    "output = nn.forward(x_train)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a569b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 20.9879, Train Acc: 0.3333, Val Loss: 18.1464, Val Acc: 0.3333\n",
      "Epoch 10/1000, Train Loss: 9.4699, Train Acc: 0.4333, Val Loss: 8.3610, Val Acc: 0.4222\n",
      "Epoch 20/1000, Train Loss: 5.4718, Train Acc: 0.4667, Val Loss: 4.8309, Val Acc: 0.4667\n",
      "Epoch 30/1000, Train Loss: 2.8478, Train Acc: 0.5571, Val Loss: 2.5543, Val Acc: 0.5556\n",
      "Epoch 40/1000, Train Loss: 1.6182, Train Acc: 0.6571, Val Loss: 1.5386, Val Acc: 0.6444\n",
      "Epoch 50/1000, Train Loss: 1.0505, Train Acc: 0.7429, Val Loss: 0.9983, Val Acc: 0.6889\n",
      "Epoch 60/1000, Train Loss: 0.7107, Train Acc: 0.8143, Val Loss: 0.6881, Val Acc: 0.7556\n",
      "Epoch 70/1000, Train Loss: 0.4883, Train Acc: 0.8714, Val Loss: 0.5226, Val Acc: 0.8222\n",
      "Epoch 80/1000, Train Loss: 0.3333, Train Acc: 0.9000, Val Loss: 0.4237, Val Acc: 0.8444\n",
      "Epoch 90/1000, Train Loss: 0.2320, Train Acc: 0.9238, Val Loss: 0.3569, Val Acc: 0.8444\n",
      "Epoch 100/1000, Train Loss: 0.1863, Train Acc: 0.9429, Val Loss: 0.3150, Val Acc: 0.8889\n",
      "Epoch 110/1000, Train Loss: 0.1570, Train Acc: 0.9571, Val Loss: 0.2790, Val Acc: 0.9111\n",
      "Epoch 120/1000, Train Loss: 0.1350, Train Acc: 0.9714, Val Loss: 0.2483, Val Acc: 0.9111\n",
      "Epoch 130/1000, Train Loss: 0.1164, Train Acc: 0.9714, Val Loss: 0.2220, Val Acc: 0.9111\n",
      "Epoch 140/1000, Train Loss: 0.1010, Train Acc: 0.9714, Val Loss: 0.2008, Val Acc: 0.9333\n",
      "Epoch 150/1000, Train Loss: 0.0885, Train Acc: 0.9810, Val Loss: 0.1842, Val Acc: 0.9333\n",
      "Epoch 160/1000, Train Loss: 0.0781, Train Acc: 0.9810, Val Loss: 0.1704, Val Acc: 0.9333\n",
      "Epoch 170/1000, Train Loss: 0.0697, Train Acc: 0.9810, Val Loss: 0.1594, Val Acc: 0.9333\n",
      "Epoch 180/1000, Train Loss: 0.0629, Train Acc: 0.9857, Val Loss: 0.1504, Val Acc: 0.9333\n",
      "Epoch 190/1000, Train Loss: 0.0573, Train Acc: 0.9905, Val Loss: 0.1428, Val Acc: 0.9333\n",
      "Epoch 200/1000, Train Loss: 0.0530, Train Acc: 0.9905, Val Loss: 0.1370, Val Acc: 0.9333\n",
      "Epoch 210/1000, Train Loss: 0.0494, Train Acc: 0.9952, Val Loss: 0.1316, Val Acc: 0.9333\n",
      "Epoch 220/1000, Train Loss: 0.0464, Train Acc: 1.0000, Val Loss: 0.1267, Val Acc: 0.9333\n",
      "Epoch 230/1000, Train Loss: 0.0438, Train Acc: 1.0000, Val Loss: 0.1223, Val Acc: 0.9333\n",
      "Epoch 240/1000, Train Loss: 0.0414, Train Acc: 1.0000, Val Loss: 0.1181, Val Acc: 0.9556\n",
      "Epoch 250/1000, Train Loss: 0.0393, Train Acc: 1.0000, Val Loss: 0.1142, Val Acc: 0.9778\n",
      "Epoch 260/1000, Train Loss: 0.0375, Train Acc: 1.0000, Val Loss: 0.1106, Val Acc: 0.9778\n",
      "Epoch 270/1000, Train Loss: 0.0358, Train Acc: 1.0000, Val Loss: 0.1073, Val Acc: 0.9778\n",
      "Epoch 280/1000, Train Loss: 0.0343, Train Acc: 1.0000, Val Loss: 0.1043, Val Acc: 0.9778\n",
      "Epoch 290/1000, Train Loss: 0.0329, Train Acc: 1.0000, Val Loss: 0.1015, Val Acc: 0.9778\n",
      "Epoch 300/1000, Train Loss: 0.0317, Train Acc: 1.0000, Val Loss: 0.0988, Val Acc: 0.9778\n",
      "Epoch 310/1000, Train Loss: 0.0305, Train Acc: 1.0000, Val Loss: 0.0964, Val Acc: 0.9778\n",
      "Epoch 320/1000, Train Loss: 0.0295, Train Acc: 1.0000, Val Loss: 0.0942, Val Acc: 0.9778\n",
      "Epoch 330/1000, Train Loss: 0.0286, Train Acc: 1.0000, Val Loss: 0.0921, Val Acc: 0.9778\n",
      "Epoch 340/1000, Train Loss: 0.0277, Train Acc: 1.0000, Val Loss: 0.0902, Val Acc: 0.9778\n",
      "Epoch 350/1000, Train Loss: 0.0268, Train Acc: 1.0000, Val Loss: 0.0883, Val Acc: 0.9778\n",
      "Epoch 360/1000, Train Loss: 0.0261, Train Acc: 1.0000, Val Loss: 0.0866, Val Acc: 0.9778\n",
      "Epoch 370/1000, Train Loss: 0.0253, Train Acc: 1.0000, Val Loss: 0.0850, Val Acc: 0.9778\n",
      "Epoch 380/1000, Train Loss: 0.0247, Train Acc: 1.0000, Val Loss: 0.0835, Val Acc: 0.9778\n",
      "Epoch 390/1000, Train Loss: 0.0240, Train Acc: 1.0000, Val Loss: 0.0821, Val Acc: 0.9778\n",
      "Epoch 400/1000, Train Loss: 0.0234, Train Acc: 1.0000, Val Loss: 0.0807, Val Acc: 0.9778\n",
      "Epoch 410/1000, Train Loss: 0.0228, Train Acc: 1.0000, Val Loss: 0.0794, Val Acc: 0.9778\n",
      "Epoch 420/1000, Train Loss: 0.0223, Train Acc: 1.0000, Val Loss: 0.0782, Val Acc: 0.9778\n",
      "Epoch 430/1000, Train Loss: 0.0218, Train Acc: 1.0000, Val Loss: 0.0770, Val Acc: 0.9778\n",
      "Epoch 440/1000, Train Loss: 0.0213, Train Acc: 1.0000, Val Loss: 0.0759, Val Acc: 0.9778\n",
      "Epoch 450/1000, Train Loss: 0.0208, Train Acc: 1.0000, Val Loss: 0.0749, Val Acc: 0.9778\n",
      "Epoch 460/1000, Train Loss: 0.0204, Train Acc: 1.0000, Val Loss: 0.0739, Val Acc: 0.9778\n",
      "Epoch 470/1000, Train Loss: 0.0200, Train Acc: 1.0000, Val Loss: 0.0730, Val Acc: 0.9778\n",
      "Epoch 480/1000, Train Loss: 0.0196, Train Acc: 1.0000, Val Loss: 0.0721, Val Acc: 0.9778\n",
      "Epoch 490/1000, Train Loss: 0.0193, Train Acc: 1.0000, Val Loss: 0.0712, Val Acc: 0.9778\n",
      "Epoch 500/1000, Train Loss: 0.0189, Train Acc: 1.0000, Val Loss: 0.0704, Val Acc: 0.9778\n",
      "Epoch 510/1000, Train Loss: 0.0186, Train Acc: 1.0000, Val Loss: 0.0696, Val Acc: 0.9778\n",
      "Epoch 520/1000, Train Loss: 0.0183, Train Acc: 1.0000, Val Loss: 0.0688, Val Acc: 0.9778\n",
      "Epoch 530/1000, Train Loss: 0.0180, Train Acc: 1.0000, Val Loss: 0.0681, Val Acc: 0.9778\n",
      "Epoch 540/1000, Train Loss: 0.0176, Train Acc: 1.0000, Val Loss: 0.0673, Val Acc: 0.9778\n",
      "Epoch 550/1000, Train Loss: 0.0173, Train Acc: 1.0000, Val Loss: 0.0667, Val Acc: 0.9778\n",
      "Epoch 560/1000, Train Loss: 0.0170, Train Acc: 1.0000, Val Loss: 0.0660, Val Acc: 0.9778\n",
      "Epoch 570/1000, Train Loss: 0.0168, Train Acc: 1.0000, Val Loss: 0.0654, Val Acc: 0.9778\n",
      "Epoch 580/1000, Train Loss: 0.0165, Train Acc: 1.0000, Val Loss: 0.0648, Val Acc: 0.9778\n",
      "Epoch 590/1000, Train Loss: 0.0162, Train Acc: 1.0000, Val Loss: 0.0643, Val Acc: 0.9778\n",
      "Epoch 600/1000, Train Loss: 0.0160, Train Acc: 1.0000, Val Loss: 0.0637, Val Acc: 0.9778\n",
      "Epoch 610/1000, Train Loss: 0.0157, Train Acc: 1.0000, Val Loss: 0.0633, Val Acc: 0.9778\n",
      "Epoch 620/1000, Train Loss: 0.0155, Train Acc: 1.0000, Val Loss: 0.0628, Val Acc: 0.9556\n",
      "Epoch 630/1000, Train Loss: 0.0152, Train Acc: 1.0000, Val Loss: 0.0624, Val Acc: 0.9556\n",
      "Epoch 640/1000, Train Loss: 0.0150, Train Acc: 1.0000, Val Loss: 0.0620, Val Acc: 0.9556\n",
      "Epoch 650/1000, Train Loss: 0.0148, Train Acc: 1.0000, Val Loss: 0.0616, Val Acc: 0.9556\n",
      "Epoch 660/1000, Train Loss: 0.0146, Train Acc: 1.0000, Val Loss: 0.0612, Val Acc: 0.9556\n",
      "Epoch 670/1000, Train Loss: 0.0143, Train Acc: 1.0000, Val Loss: 0.0608, Val Acc: 0.9556\n",
      "Epoch 680/1000, Train Loss: 0.0141, Train Acc: 1.0000, Val Loss: 0.0604, Val Acc: 0.9556\n",
      "Epoch 690/1000, Train Loss: 0.0139, Train Acc: 1.0000, Val Loss: 0.0601, Val Acc: 0.9556\n",
      "Epoch 700/1000, Train Loss: 0.0138, Train Acc: 1.0000, Val Loss: 0.0597, Val Acc: 0.9556\n",
      "Epoch 710/1000, Train Loss: 0.0136, Train Acc: 1.0000, Val Loss: 0.0594, Val Acc: 0.9556\n",
      "Epoch 720/1000, Train Loss: 0.0134, Train Acc: 1.0000, Val Loss: 0.0591, Val Acc: 0.9556\n",
      "Epoch 730/1000, Train Loss: 0.0132, Train Acc: 1.0000, Val Loss: 0.0587, Val Acc: 0.9556\n",
      "Epoch 740/1000, Train Loss: 0.0130, Train Acc: 1.0000, Val Loss: 0.0584, Val Acc: 0.9778\n",
      "Epoch 750/1000, Train Loss: 0.0129, Train Acc: 1.0000, Val Loss: 0.0580, Val Acc: 0.9778\n",
      "Epoch 760/1000, Train Loss: 0.0127, Train Acc: 1.0000, Val Loss: 0.0577, Val Acc: 0.9778\n",
      "Epoch 770/1000, Train Loss: 0.0126, Train Acc: 1.0000, Val Loss: 0.0574, Val Acc: 0.9778\n",
      "Epoch 780/1000, Train Loss: 0.0124, Train Acc: 1.0000, Val Loss: 0.0571, Val Acc: 0.9778\n",
      "Epoch 790/1000, Train Loss: 0.0123, Train Acc: 1.0000, Val Loss: 0.0568, Val Acc: 0.9778\n",
      "Epoch 800/1000, Train Loss: 0.0121, Train Acc: 1.0000, Val Loss: 0.0565, Val Acc: 0.9778\n",
      "Epoch 810/1000, Train Loss: 0.0120, Train Acc: 1.0000, Val Loss: 0.0562, Val Acc: 0.9778\n",
      "Epoch 820/1000, Train Loss: 0.0118, Train Acc: 1.0000, Val Loss: 0.0559, Val Acc: 0.9778\n",
      "Epoch 830/1000, Train Loss: 0.0117, Train Acc: 1.0000, Val Loss: 0.0557, Val Acc: 0.9778\n",
      "Epoch 840/1000, Train Loss: 0.0116, Train Acc: 1.0000, Val Loss: 0.0554, Val Acc: 0.9778\n",
      "Epoch 850/1000, Train Loss: 0.0115, Train Acc: 1.0000, Val Loss: 0.0551, Val Acc: 0.9778\n",
      "Epoch 860/1000, Train Loss: 0.0113, Train Acc: 1.0000, Val Loss: 0.0549, Val Acc: 0.9778\n",
      "Epoch 870/1000, Train Loss: 0.0112, Train Acc: 1.0000, Val Loss: 0.0546, Val Acc: 0.9778\n",
      "Epoch 880/1000, Train Loss: 0.0111, Train Acc: 1.0000, Val Loss: 0.0544, Val Acc: 0.9778\n",
      "Epoch 890/1000, Train Loss: 0.0110, Train Acc: 1.0000, Val Loss: 0.0541, Val Acc: 0.9778\n",
      "Epoch 900/1000, Train Loss: 0.0109, Train Acc: 1.0000, Val Loss: 0.0539, Val Acc: 0.9778\n",
      "Epoch 910/1000, Train Loss: 0.0108, Train Acc: 1.0000, Val Loss: 0.0537, Val Acc: 0.9778\n",
      "Epoch 920/1000, Train Loss: 0.0107, Train Acc: 1.0000, Val Loss: 0.0535, Val Acc: 0.9778\n",
      "Epoch 930/1000, Train Loss: 0.0106, Train Acc: 1.0000, Val Loss: 0.0533, Val Acc: 0.9778\n",
      "Epoch 940/1000, Train Loss: 0.0105, Train Acc: 1.0000, Val Loss: 0.0531, Val Acc: 0.9778\n",
      "Epoch 950/1000, Train Loss: 0.0104, Train Acc: 1.0000, Val Loss: 0.0529, Val Acc: 0.9778\n",
      "Epoch 960/1000, Train Loss: 0.0103, Train Acc: 1.0000, Val Loss: 0.0527, Val Acc: 0.9778\n",
      "Epoch 970/1000, Train Loss: 0.0102, Train Acc: 1.0000, Val Loss: 0.0525, Val Acc: 0.9778\n",
      "Epoch 980/1000, Train Loss: 0.0101, Train Acc: 1.0000, Val Loss: 0.0523, Val Acc: 0.9778\n",
      "Epoch 990/1000, Train Loss: 0.0100, Train Acc: 1.0000, Val Loss: 0.0522, Val Acc: 0.9778\n",
      "Epoch 1000/1000, Train Loss: 0.0099, Train Acc: 1.0000, Val Loss: 0.0520, Val Acc: 0.9778\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = nn.fit(x_train, y_train, x_val, y_val, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ccd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzYAAAGsCAYAAAAPLTJNAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPClJREFUeJzt3Qd8W9XZx/HnSrI84tjZcZw4k5CEACHsMFp4k5KEUVbLKIVQKHRAKQQKpGVTCGWVssLbAZQOVl8ILSMUAiSMEAgQwgyZZDoTr3hp3PfzHFmK5NiOZGtc2b/vp+qVrq6urqSLor/POc+xbNu2BQAAAACymCvTBwAAAAAAHUWwAQAAAJD1CDYAAAAAsh7BBgAAAEDWI9gAAAAAyHoEGwAAAABZj2ADAAAAIOt5xGGCwaBs2LBBunfvLpZlZfpwAAAAAGSITrlZXV0tpaWl4nK5sivYaKgpKyvL9GEAAAAAcIi1a9fKoEGDsivYaEtN+OCLiooyfTgAAAAAMqSqqso0eoQzQlYFm3D3Mw01BBsAAAAAVhxDVCgeAAAAACDrEWwAAAAAZD2CDQAAAICs57gxNgAAAHCmQCAgPp8v04eBTsbr9e62lHM8CDYAAADY7Vwi5eXlUlFRwTuFpNNQM2zYMBNwOoJgAwAAgDaFQ02/fv2koKCASdSRNMFg0MxjuXHjRhk8eHCHzi2CDQAAANrsfhYONb179+adQtL17dvXhBu/3y85OTnt3g/FAwAAANCq8JgabakBUiHcBU1DdEcQbAAAALBbHekiBKTj3CLYAAAAAMh6BBsAAAAAWY9gAwAAAMRh6NChcs899/BeORTBBgAAAJ1uzEZblxtuuKFd+33//fflwgsv7NCxHXXUUXLppZd2aB9oGeWe27C+ok5Wbdkhfbp7ZXRJUVubAgAAwCF0TpSwJ598Uq677jpZunRpZF1hYWHM5KNajcvj8cRVlhjORYtNG/7vg3Xyw78slL++szp9nwgAAICDaRCobfRn5KLPHY+SkpLIpbi42LTShG9/+eWX0r17d3nppZfkgAMOkNzcXHnrrbdkxYoVcuKJJ0r//v1N8DnooIPk1VdfbbMrmu73z3/+s5x88smmHPbIkSPl3//+d4fe3//7v/+TsWPHmuPS57vrrrti7n/wwQfN8+Tl5Zlj/d73vhe571//+pfss88+kp+fb+YcmjRpkuzYsUO6Clps2lCYG3p7qur96fo8AAAAHK3OF5C9rns5I8/9+U2TpcCbnJ+vV199tdx5550yfPhw6dmzp6xdu1aOPfZYueWWW0yoeOyxx+SEE04wLT2DBw9udT833nij3H777XLHHXfIfffdJ2eddZZ8/fXX0qtXr4SP6YMPPpDTTjvNdJU7/fTT5Z133pGf//znJqSce+65smjRIrnkkkvkb3/7mxx22GGyfft2efPNNyOtVGeeeaY5Fg1a1dXV5r54w2BnQLBpQ/e80NtTTbABAADoVG666Sb5zne+E7mtQWTcuHGR2zfffLM8++yzpgXm4osvbnU/Gjg0UKhbb71V7r33XnnvvfdkypQpCR/T3XffLRMnTpRrr73W3N5zzz3l888/N6FJn2fNmjXSrVs3Of74402r05AhQ2T8+PGRYOP3++WUU04x65W23nQlBJs2dM/LMcvq+tCMuwAAAF1dfo7btJxk6rmT5cADD4y5XVNTY1pKXnjhhUhIqKurM2GiLfvuu2/kuoaOoqIi2bx5c7uO6YsvvjDd4aIdfvjhpvubjgPSIKahRVuZNDjpJdwNbty4cSYUaZiZPHmyHHPMMaabmrZGdRWMsWlDES02AAAAMXRciXYHy8QlWTPUh0NItCuuuMK00Giri3bhWrx4sQkJjY2Nbe4nJydnl/cnGAym5KzRVpoPP/xQHn/8cRkwYIApiqCBpqKiQtxut7zyyitm7NBee+1lusWNGjVKVq1aJV0FwaYNtNgAAAB0DW+//bbp7qUtIBpotNDA6tXpLSA1ZswYcxzNj0u7pGlwUVq9TYsC6FiaJUuWmGN87bXXIqFKW3h03M9HH30kXq/XhLWugq5ocYyxqWGMDQAAQKemlcaeeeYZUzBAA4KOc0lVy8uWLVtMi1A0bYG5/PLLTTU2Hd+jxQMWLFgg999/v6mEpp5//nlZuXKlfOtb3zJdzF588UVzjNoys3DhQpk7d67pgtavXz9zW59Hw1JXQbCJI9jsaAxIIGiL25W85k8AAAA4hw7cP++880y1sT59+shVV10lVVVVKXmuf/7zn+YSTcPMNddcI0899ZTpYqa3NexokQNtSVI9evQw4UvHAtXX15swpt3StDz0F198IfPnzzfjcfS4dSyOloqeOnWqdBWW7bAacPpBaL3xyspKM/gqkxr9QdnzmpfM9Y+vO0aKC2L7UAIAAHR2+gNax2kMGzbMzJ0CpPMcSyQbMMamDV6PS3I9obeoispoAAAAgGMRbHaDuWwAAAAA5yPY7AaV0QAAAADnI9jsBi02AAAAQCcLNjNnzjQl6HRyIC0jd9JJJ8nSpUt3Gfxz0UUXSe/evaWwsFBOPfVU2bRpk2Srbt5QZbRaXyDThwIAAAAgGcFm3rx5JrS8++67ZmZTn89namXv2LEjss1ll10m//nPf+Tpp58222/YsEFOOeUUyVb53tBkSPWNBBsAAACgU8xjM2fOnJjbjz76qGm5+eCDD8xEQVqG7S9/+Yupy/0///M/ZptHHnnETAykYejQQw+VbJOfEwo2tY3+TB8KAAAAgFSMsdEgo3r16mWWGnC0FWfSpEmRbUaPHi2DBw82M6e2pKGhwdSnjr44scWmzpeamWcBAAAAZDDYBINBufTSS+Xwww+Xvffe26wrLy8Xr9drZkWN1r9/f3Nfa+N2dNKd8KWsrEyc2GJTxxgbAACALuWoo44yv3fDhg4dKvfcc0+bj7EsS2bPnt3h507WfrqSdgcbHWvz6aefyhNPPNGhA5gxY4Zp+Qlf1q5dK44cY0OwAQAAyAonnHCCTJkypcX73nzzTRMalixZkvB+33//fbnwwgslmW644QbZb7/9dlm/ceNGmTp1qqTSo48+ukuDRJcZYxN28cUXy/PPPy/z58+XQYMGRdaXlJRIY2OjVFRUxLxJWhVN72tJbm6uuThVHmNsAAAAssr5559vKvOuW7cu5rdqePz3gQceKPvuu2/C++3bt6+kS2u/nZGkFhvbtk2oefbZZ+W1116TYcOGxdx/wAEHSE5OjsydOzeyTstBr1mzRiZMmCDZqCA8xqaRMTYAAADZ4PjjjzchRFskotXU1JjKvRp8tm3bJmeeeaYMHDhQCgoKZJ999pHHH3+8zf0274q2bNkyU0ArLy9P9tprL1M1uLmrrrpK9txzT/Mcw4cPl2uvvdaMSVd6fDfeeKN8/PHHphVJL+Fjbt4V7ZNPPjHFufLz8820KtpypK8n7NxzzzVTsdx5550yYMAAs432sAo/V3vob/gTTzzRTOFSVFQkp512Wsw0LnrcRx99tJkKRu/XLLBo0SJz39dff21aznr27CndunWTsWPHyosvviiOabHRN0crnj333HPmBYTHzejYGH2TdaknyvTp001BAX2Bv/jFL0yoycaKaNFjbOiKBgAAYP7SLeKrzcxbkVOgv/h3u5nH45FzzjnHhITf/OY3JiQoDTWBQMAEGg0F+kNcg4f+Zn3hhRfk7LPPlhEjRsjBBx8c13hzndJEx5IvXLjQDKmIHo8Tpr+Z9ThKS0tNOLngggvMuiuvvFJOP/10M7RDKw+/+uqrZnv9Pd2cTq0yefJk85tau8Nt3rxZfvzjH5sGh+jw9vrrr5tQo8vly5eb/Ws3N33OROnrC4cancLF7/ebLKD7fOONN8w2Z511lowfP15mzZolbrdbFi9ebBo5lG6rPbm0h5cGm88//9zsyzHBRg86PJCqeZOepkT1+9//Xlwul2n+04pn+iE8+OCDkq0oHgAAABBFQ82tpZl5S369QcTbLa5NzzvvPLnjjjvMj/Lwb1f9zaq/UcNFq6644orI9vrH+JdfflmeeuqpuIKNBpEvv/zSPEZDi7r11lt3GRdzzTXXxLT46HPqGHUNNtowoD/2NYi11fVMGxbq6+vlscceMyFB3X///aZF5He/+50JV0pbR3S9hgytTHzccceZnlTtCTb6OA1iq1atihT30ufXlhcNVwcddJBp0fnVr35lnkuNHDky8ni9T99rbQlT2lrluK5oLV3CoUZpU9wDDzwg27dvN+nymWeeyeo+gnmRrmhM0AkAAJAt9Mf2YYcdJg8//LC5rS0YWjhAexcpbbm5+eabzQ9v7WmkAUNDiv4gj8cXX3xhfvCHQ41qaejFk08+aaoI6+9hfQ4NOvE+R/RzjRs3LhJqlO5TW1V02EfY2LFjTagJ09Ybbd1pj/Dri65YrN3tdBy93qe0l5a2HOlUL7fddpusWLEisu0ll1wiv/3tb81xXn/99e0q1pCW4gFdSUG4eABV0QAAAELdwbTlJFPPnQANMdoSo39019Ya7Wb27W9/29ynrTl/+MMfzJgZDTcaGrQrmXafShadx1G7a+k4Gu3FpK1E2lpz1113SSrkNHUDC9MueBp+UkUruv3gBz8w3fheeuklE2D09Z188skm8Ohr1vv++9//mile9HXr5+HICTq7gki5Z1psAAAAQmNctDtYJi5xjK+JpoPddYiEduXSblTaPS083ubtt982Y0h++MMfmtYQ7Sr11Vdfxb3vMWPGmGlKtCxz2LvvvhuzzTvvvCNDhgwx43y0Ept21dJB9dF0DkhtPdrdc+lAfe0NFabHr69t1KhRkgpjml5f9FQsOk5Gqx9ry02YFka47LLLTHjRMUcaIMO0teenP/2p6cF1+eWXy5/+9CdJJYJNnOWemaATAAAgu2jXLx3srvMmagCJHj6hIUOrmGn40K5VP/nJT2Iqfu2Odr/SH/XTpk0zoUO7uWmAiabPod3OtBVDu2nde++9prpwNB13o+NYdOD91q1bzRj15rTVR4d76HNpsQEtDqAtH1rsIDy+pr00VOlzR1/0/dDXpy1Z+twffvihvPfee6Ygg7Z4aUirq6szxQu0kICGNQ1aOvZGA5HS1i/t2qevTR+vxxy+L1UINrtB8QAAAIDspd3RvvnmG9MtKno8jI512X///c16LS6gY2C0XHK8tLVEQ4r+wNdiA9r16pZbbonZ5rvf/a5pzdAAoNXJNERpuedoOsBeJxPVsslaorqlktNaKlpDgo5h10H73/ve92TixImmUEBH1dTUmMpm0RctSqAtW1oJWQsSaElrDTraqqVjhpSO5dGS2Rp2NOBp65gWTtBud+HApJXRNMzo69NtUl1QzLJ19L+DVFVVmf6HWjJPS+9l2uqtO+SoO9+QwlyPfHrj5EwfDgAAQFppNS79q7vOX6itBkA6z7FEsgEtNnGOsdGuaA7LgAAAAACaEGziHGMTCNrSGEhdVQkAAAAA7Uew2Y1cz863qNFPsAEAAACciGCTQLBpINgAAAAAjkSw2Q2tCOF1h94mWmwAAAAAZyLYJNBqQ4sNAADoqlI5gz26NjtJBbo8SdlLJ5eb45LqBg02bc8KCwAA0Nl4vV4zZ8uGDRvMPCt6W3u0AMkKNVu2bDHnVE5OTof2RbCJA13RAABAV6WhRucX2bhxowk3QLJpqBk0aJCZ9LMjCDZxyG0q+UxXNAAA0BVpK83gwYPF7/ebGeWBZNKWmo6GGkWwSWSMjY++pQAAoGsKdxXqaHchIFUoHhAHb1OwaeQvFAAAAIAjEWziQIsNAAAA4GwEmzjkehhjAwAAADgZwSaRrmh+xtgAAAAATkSwSWiCTqqAAAAAAE5EsEko2NBiAwAAADgRwSaBrmgEGwAAAMCZCDZxoHgAAAAA4GwEm7a8/xeRe8fLlPKHzE3G2AAAAADORLBpS0O1yPaVUuTfbm5SFQ0AAABwJoJNWzy5ZuEVn1kyxgYAAABwJoJNIsHGR1U0AAAAwIkINm1xh4KNx240y8YAwQYAAABwIoJNHC02OZEWGyboBAAAAJyIYBNHsPEEQy02jLEBAAAAnIlgE1dXtFCLDVXRAAAAAGci2LTF4zULd6TFhq5oAAAAgBMRbNriyWsWbCgeAAAAADgRwaYt7tgWG7qiAQAAAM5EsImjxcZFiw0AAADgaASbOKqiuQKMsQEAAACcjGATR1c0i65oAAAAgKMRbOLpihZoEBGb4gEAAACAQxFs4ij3rLziJ9gAAAAADkWwiWOCTuUVnwSCtvgDlHwGAAAAnIZgE0fxAJUrPrNsJNgAAAAAjkOwaYtlRQoIaFc01eCjxQYAAABwGoJNnN3RCly02AAAAABORbCJsztaN0/ALGmxAQAAAJyHYBNnsCl0NwUbf2gJAAAAwDkINgkHG8bYAAAAAE5DsIl3jA3BBgAAAHAsgk2ck3QWuOiKBgAAADgVwWZ3PHlm0c3dVBWNrmgAAACA4xBsdqdpHpu8SIsNY2wAAAAApyHYxFk8YGdXNIINAAAA4DQEmzi7ouWHJ+gk2AAAAACOQ7CJtyua5TdL5rEBAAAAnIdgE2dXtEiw8dEVDQAAAHAagk2CwaYxQLABAAAAnIZgE+cEnblWaIwNLTYAAACA8xBs4myxiQQbf6g6GgAAAADnINjEG2ykqSsaVdEAAAAAxyHYxNkVLUfCLTaMsQEAAACchmATZ4uNNxJs6IoGAAAAOA3BJt5gYzNBJwAAAOBUBJs4J+ikKxoAAADgXASb3fHkhRZ2o1kyxgYAAABwHoLN7nhCLTYeuqIBAAAAjkWwibMqmifYYJYUDwAAAACch2ATd1c0yj0DAAAATkWwibMrmjtIVTQAAADAqQg2cbbYuCJd0ZigEwAAAHAagk2c5Z5dgaaqaD4m6AQAAACchmAT5wSdrmAo2DQGaLEBAAAAnIZgE2+wCTR1RfMRbAAAAACnIdjEWe7ZCndFY4wNAAAA4DgEmzhbbKygTywJmq5otm2n4aMBAAAAkLJgM3/+fDnhhBOktLRULMuS2bNnx9x/7rnnmvXRlylTpki2BxvlFb9Z0moDAAAAZHmw2bFjh4wbN04eeOCBVrfRILNx48bI5fHHH5ds74qmcoVJOgEAAAAn8iT6gKlTp5pLW3Jzc6WkpEQ6BXdO5Gqu5RexRRoZZwMAAAB0/jE2b7zxhvTr109GjRolP/vZz2Tbtm2tbtvQ0CBVVVUxF0exrMgknYXucFc05rIBAAAAOnWw0W5ojz32mMydO1d+97vfybx580wLTyDQchiYOXOmFBcXRy5lZWXi1O5o3Tyh18AYGwAAACDLu6LtzhlnnBG5vs8++8i+++4rI0aMMK04EydO3GX7GTNmyPTp0yO3tcXGceFGCwg0iHRzhVps/AGqogEAAABdqtzz8OHDpU+fPrJ8+fJWx+MUFRXFXJxaGa3AFWqx8QWYpBMAAADoUsFm3bp1ZozNgAEDJGu5vWaR3zTGRueyAQAAAJDFXdFqampiWl9WrVolixcvll69epnLjTfeKKeeeqqpirZixQq58sorZY899pDJkydL1moqHlDQ1BXNR1U0AAAAILuDzaJFi+Too4+O3A6Pj5k2bZrMmjVLlixZIn/961+loqLCTOJ5zDHHyM0332y6nGUtjzemK5o/yBgbAAAAIKuDzVFHHSW23foP+5dfflk6naaqaHk6jw1d0QAAAICuN8amU2gqHpDv8pklXdEAAAAAZyHYJBBs8qxwVTS6ogEAAABOQrBJoCpaXrh4AFXRAAAAAEch2CRQFS1PQl3RKPcMAAAAOAvBJoGuaF6raYwNLTYAAACAoxBsEhpjwzw2AAAAgBMRbBIo9+xt6opG8QAAAADAWQg2CUzQmcsYGwAAAMCRCDYJFA8It9j4KfcMAAAAOArBJoFyz16h3DMAAADgRASbBIoH5ETG2ART+qEAAAAASAzBJpGuaHajWTKPDQAAAOAsBJsEuqJ5aLEBAAAAHIlgk0hXNLupK5rfTumHAgAAACAxBJsEgo2nqSsaY2wAAAAAZyHYJDBBp6epxYYxNgAAAICzEGwSmKDTE6TFBgAAAHAigk0CVdF2BhvG2AAAAABOQrBJoCuaO1w8gHlsAAAAAEch2CRQPMAdbDBLgg0AAADgLASbBIKNKxhusaErGgAAAOAkBJsEJuh0UTwAAAAAcCSCTQLFA9yBUFe0Rn8wpR8KAAAAgMQQbBLoimbZAXFJkDE2AAAAgMMQbBLoiqa84mOMDQAAAOAwBJsEuqKpXBNs6IoGAAAAOAnBJh5uj4jlimqxIdgAAAAATkKwSXCSzlzLR/EAAAAAwGEINgkWENCuaP4g89gAAAAATkKwSTDYeMVPVzQAAADAYQg27Wix8QVssW1abQAAAACnINgkOMZGiwcoDTcAAAAAnIFgk2hXNMtvllRGAwAAAJyDYJNwV7RGsyTYAAAAAM5BsEm43HOoxaaRuWwAAAAAxyDYxMvjNYt8V8AsGWMDAAAAOAfBJl6ePLPo5moaY+MPpuxDAQAAAJAYgk283KEWm7xwsKErGgAAAOAYBJsEiwcUWJR7BgAAAJyGYJNgi43XFeqCRosNAAAA4BwEm3i5c8wiL1I8gDE2AAAAgFMQbBJssaHcMwAAAOA8BJtEu6JZ4a5odso+FAAAAACJIdgk2BXN2zRBJ+WeAQAAAOcg2CTcFY0xNgAAAIDTEGwSbbGRUItNI8UDAAAAAMcg2CTYYpMT7orGGBsAAADAMQg2cb9T4RYbuqIBAAAATkOwSbArWrjFxk9XNAAAAMAxCDaJdkWLjLGh3DMAAADgFASbdgYbHy02AAAAgGMQbBLsiuYJj7HxhybqBAAAAJB5BJtEW2xsn1nSYgMAAAA4B8EmwWDjYYwNAAAA4DgEm0S7otmhMTYN/lCXNAAAAACZR7BpZ4tNvY8xNgAAAIBTEGwSbrEJjbGpawwFHAAAAACZR7BJMNi4m7qi1fnoigYAAAA4BcEmwa5o7nCLDV3RAAAAAMcg2LQ32NAVDQAAAHAMgk2CXdFcQbqiAQAAAE5DsEmwxcYVDLfYMMYGAAAAcAqCTYLBxjLBxibYAAAAAA5CsEmwK5oltrglSFU0AAAAwEEINgm22Kgc8RNsAAAAAAch2MT9ToVabJRX/FLvC0owaKfoYwEAAACQCIJNgl3Rwi02qt5PAQEAAADACQg28bKsSKtNONhQGQ0AAABwBoJNO8bZFHpCLTW1lHwGAAAAHIFgkwhPrlkU5wTNst5HVzQAAADACQg2icjJN4uiphabOoINAAAA4AgEm0R48syiuyc0xoauaAAAAECWBpv58+fLCSecIKWlpWJZlsyePTvmftu25brrrpMBAwZIfn6+TJo0SZYtWyadKdj0yAm12OxoCAUcAAAAAFkWbHbs2CHjxo2TBx54oMX7b7/9drn33nvloYcekoULF0q3bt1k8uTJUl9fL1kvJxRsippabGoINgAAAIAjeBJ9wNSpU82lJdpac88998g111wjJ554oln32GOPSf/+/U3LzhlnnCFZzRMeY0OwAQAAADrtGJtVq1ZJeXm56X4WVlxcLIcccogsWLCgxcc0NDRIVVVVzMXpLTbd3U3Bpp6uaAAAAECnCzYaapS20ETT2+H7mps5c6YJP+FLWVmZOH2MTWFTsGGMDQAAAOAMGa+KNmPGDKmsrIxc1q5dK04v99zN7TPLasbYAAAAAJ0v2JSUlJjlpk2bYtbr7fB9zeXm5kpRUVHMxektNt1coWBDVzQAAACgEwabYcOGmQAzd+7cyDodM6PV0SZMmCBZr6nFJt9qCja02AAAAADZWRWtpqZGli9fHlMwYPHixdKrVy8ZPHiwXHrppfLb3/5WRo4caYLOtddea+a8OemkkyTrNbXY5EujWRJsAAAAgCwNNosWLZKjjz46cnv69OlmOW3aNHn00UflyiuvNHPdXHjhhVJRUSFHHHGEzJkzR/LyQqGgM7TY5FkEGwAAACCrg81RRx1l5qtpjWVZctNNN5lLp9PUYpMbbrGh3DMAAADgCBmvipaNLTZemxYbAAAAwEkINu1osckJNpglLTYAAACAMxBs2tFik2M3ROaxCQRb75YHAAAAID0INu1osfE0tdio6vpQ6WcAAAAAmUOwaUeLjSvQIIW5oboLFbUEGwAAACDTCDbtaLERX50U5+eYqxV1BBsAAAAg0wg2iejWJ7Ss2Sw9CpqCTW2oQhoAAACAzCHYJKLHkNCybrsMyAsFmkpabAAAAICMI9gkIrdQpFtfc3W4e4tZMsYGAAAAyDyCTaJ6DjWLwS6CDQAAAOAUBJt2BpuBdrlZVtQxxgYAAADINIJNonoOM4sS3zqzrKTcMwAAAJBxBJtE9R1lFv3qV5nllpqdk3UCAAAAyAyCTaL67WUWxTUrRMSWjZX1KfhYAAAAACTCk9DWEOm9h4jLIx5fjZTIdtlY4RHbtsWyLN4dAAAAIENosUmUxyvSa4S5OtK1XnY0BqSq3p+CjwYAAABAvAg27dE7FGzG5G41y3K6owEAAAAZRbBpj17DzWKMNzSXzYbKuqR+KAAAAAASQ7DpQLAZ5tpklhsrKCAAAAAAZBLBpgNd0QYGN5jlRlpsAAAAgIwi2HSgxaZX4wZxSZCSzwAAAECGEWzao2iQiDtX3LZfSq2ttNgAAAAAGUawade75hLpOdRcHWptYowNAAAAkGEEmw52RxtqlZuuaDpJJwAAAIDMINh0sICABps6X0Aq63xJ/FgAAAAAJIJg08EWm9GeUMnnDZR8BgAAADKGYNNefUebxR6u9WZZXsUknQAAAECmEGzaq98YsygJbpICqafFBgAAAMgggk17FfQS6dbXXN3DWk/JZwAAACCDCDbJ6I6mwYYxNgAAAEDGEGw6otcwsxjs2mxKPgMAAADIDIJNR/QYYhZl1ha6ogEAAAAZRLDpiJ5DzWKQCTZM0gkAAABkCsEmKS02m6XBH5RvapmkEwAAAMgEgk1H9AwFmxLrG/GKTzZUMJcNAAAAkAkEm47Qcs85BeISW0qtrRQQAAAAADKEYNMRliXSY7C5OsjaKuWVtNgAAAAAmUCwSeI4mw2UfAYAAAAygmCTpHE2puQzY2wAAACAjCDYdBQtNgAAAEDGEWyS2GJTTlc0AAAAICMINh0VKR4QCjbBoJ2EjwUAAABAIgg2SeqK1seqEk+gVrbtaOQMBAAAANKMYNNR+T1E8opjWm0AAAAApBfBJukFBJjLBgAAAEg3gk0yUPIZAAAAyCiCTTIUl5lFibVdNlbRFQ0AAABIN4JNMhT2N4u+VoVsrCDYAAAAAOlGsEmG7iVm0U8qZCNjbAAAAIC0I9gkQ2E/s+hrVcoGWmwAAACAtCPYJLkr2qYqJukEAAAA0o1gkwyFoa5ova1qsYI+2VrTkJTdAgAAAIgPwSYZ8nuKuDzmam+plA1M0gkAAACkFcEmKe+iK9IdrZ9VIeUUEAAAAADSimCT9AICFRQQAAAAANKMYJPkcTbaYkPJZwAAACC9CDbJbrGRStnIGBsAAAAgrQg2yZ6k0/qGYAMAAACkGcEmBZN0bqyoS9puAQAAAOwewSbJY2zMJJ3VDRII2knbNQAAAIC2EWySJarcs4aaLdVM0gkAAACkC8EmWYoGmEV/q0JcEpQNzGUDAAAApA3BJpld0Sy35Ihf+mhltIr6pO0aAAAAQNsINsni9oh0D7XalFrbmMsGAAAASCOCTTIVDzSLUmsrJZ8BAACANCLYJFPxILMYQIsNAAAAkFYEm2QqCrfYbKfFBgAAAEgjgk0KWmxMVzSKBwAAAABpQ7BJUVe0zdX14gsEk7p7AAAAAC0j2KSoK1rQFibpBAAAANKEYJOCFpt+VoV4xcc4GwAAACBNCDbJVNBbxJNnrvY3BQTqkrp7AAAAAC0j2CSTZUW6ow20tkl5ZX1Sdw8AAAAgTcHmhhtuEMuyYi6jR4+WLldAQLbJBiqjAQAAAGnhScVOx44dK6+++urOJ/Gk5GmcqbjMLAZaW2VlFV3RAAAAgHRISeLQIFNSUiJdUo+yyFw2b9MVDQAAAMjeMTbLli2T0tJSGT58uJx11lmyZs2aVrdtaGiQqqqqmEtn6Io2iEk6AQAAgOwNNocccog8+uijMmfOHJk1a5asWrVKjjzySKmurm5x+5kzZ0pxcXHkUlYWavHI9q5opU2TdPqZpBMAAABIOcu2bTuVT1BRUSFDhgyRu+++W84///wWW2z0EqYtNhpuKisrpaioSLLOthUi9+0vdbZXxjQ8IgtmTJQBxfmZPioAAAAg62g20MaPeLJBykf19+jRQ/bcc09Zvnx5i/fn5uaaS6fR1BUt32qUnlJtKqMRbAAAAIAsn8empqZGVqxYIQMGDJAuwZMrUtg/0h2NuWwAAACALAw2V1xxhcybN09Wr14t77zzjpx88snidrvlzDPPlC6jaZyNFhBYX1Gb6aMBAAAAOr2kd0Vbt26dCTHbtm2Tvn37yhFHHCHvvvuuud5laHe09YtMyec12wk2AAAAQNYFmyeeeCLZu8zauWx0ks552wg2AAAAQNaPsemSincGG1psAAAAgNQj2KRCjyFmMdjaLOu/qWMuGwAAACDFCDap0HsPsxhqlYs/GDQlnwEAAACkDsEmFXoOEbHc0s1qkH5SIV9v35GSpwEAAAAQQrBJBXdOKNyIyHDXRvmaAgIAAABAShFs0tAdjQICAAAAQGoRbFKl1wizGGZpiw1d0QAAAIBUItikSu9QsBluldMVDQAAAEgxgk2Kg412RVu5ZQclnwEAAIAUItikeIzNEGuT+AN+WU13NAAAACBlCDapUjRIxJ0rXssvg6wtsrS8JmVPBQAAAHR1BJuUvbMukb6jzNXR1hpZWl6VsqcCAAAAujqCTSr139ssxmiw2VSd0qcCAAAAujKCTSqVhILNaNca+WoTXdEAAACAVCHYpFL/sZGuaFo8oK4xkNKnAwAAALoqgk0auqINcW2WfLtelm2mOxoAAACQCgSbVOrWR6Swv7jEllHWWvlkfWVKnw4AAADoqgg26eqO5lojH6+tSPnTAQAAAF0RwSZN3dH2tlbLx2tpsQEAAABSgWCTagP3N4txrhXy1eZqqWnwp/wpAQAAgK6GYJNqAw+MdEXLtRvkU8bZAAAAAElHsEm14kEihSXikaDsba2SJesYZwMAAAAkG8Em1SxLZFCo1WY/1wr5aA3BBgAAAEg2gk06DDzALPZzLZf3V28X27bT8rQAAABAV0GwSYemFpv9Xctla02jrNy6Iy1PCwAAAHQVBJt0KN1fxOWRUmubDLK2yPurtqflaQEAAICugmCTDrmFoXAjIoe6Ppf3CDYAAABAUhFs0mXYkWYxwfW5LCTYAAAAAElFsEmXoUdEgs36ilpZu702bU8NAAAAdHYEm3QpO0TElWPG2ZRZm2X+si1pe2oAAACgsyPYpIu3W6Tss7bavPnV1rQ9NQAAANDZEWwyMM7mUNcX8vaKreIPBNP69AAAAEBnRbDJwDibw92fS3W9Tz5eV5HWpwcAAAA6K4JNusfZuHOlv2yXEdYGmU93NAAAACApCDbplJMvMvRwc/Vo12IKCAAAAABJQrBJt5GTI8Hm47UVUlnrS/shAAAAAJ0NwSbdRn7HLA5xfykFdi2tNgAAAEASEGzSrfcIkV4jxCMBOcL1qbz+5ea0HwIAAADQ2RBsMmHUVLM41r1QXl+6WQJBOyOHAQAAAHQWBJtM2PsUs/iO+wNpqK2WxWsp+wwAAAB0BMEmE0r3N93R8qVRjnEtkle/2JSRwwAAAAA6C4JNJliWyD7fN1dPdr8lz320XoJ0RwMAAADajWCTKfueJrZY8m33EulWtVwWrNyWsUMBAAAAsh3BJlN6jxBrzPHm6s89z8m/PliXsUMBAAAAsh3BJpOOvNwsvut6Rz755CPZVtOQ0cMBAAAAshXBJpNKx4u9xyRxW7acJ/+WR95endHDAQAAALIVwSbDrCOvMMvvuefJ6wsWSHW9L9OHBAAAAGQdgk2mDZkg9ohJ4rUCcmngMfnzm6syfUQAAABA1iHYOIA15VYJWm4zYecn82fL5qr6TB8SAAAAkFUINk7Qd5RYB51vrl5pPSZ3v/xZpo8IAAAAyCoEG4ewjpoh/tweMtq1VvosniXvrNia6UMCAAAAsgbBxikKeonn2NvN1Us8z8gfn/6P1Db6M31UAAAAQFYg2DjJvqeJb48pppDA5bX3yF1z6JIGAAAAxINg4ySWJTkn/kF83mLZx7VaSt67Tf77WXmmjwoAAABwPIKN03QvkZyTHzBXL/C8KC8+9UdZvXVHpo8KAAAAcDSCjRONOUECh15srt4sD8otf50tVUzcCQAAALSKYONQ7u/cII0DD5XuVp1cX3Wd/Orhl6XeF8j0YQEAAACORLBxKneOeH/wT2koGiaDrK3yi/LfyBV/f0sa/cFMHxkAAADgOAQbJ+vWW3LPfUZ8eb1lb9dqOX/VdLno4blSTbc0AAAAIAbBxul6DZecc54Rn7eHjHctl+nrpsuFs16Sdd/UZvrIAAAAAMcg2GSD0v0k5/yXxJffV8a41shdFb+Uq+99RF5fujnTRwYAAAA4AsEmW/TfS3J+/LL4eo6QUmu7/CV4ncx/7Ca5c84XEgjamT46AAAAIKMINtmk9wjJ+ckbEhh1vORafrk+52/yrXemyVUPPS1bqhsyfXQAAABAxhBssk1ekbjP+LvIsXeK350vB7uWym2bfiJv/P4ceeW9T8S2ab0BAABA10OwyUaWJXLwBeK5eKHsGDJJPFZQvh+cI0e+cLS8fucPZPlnizJ9hAAAAEBaWbbD/sRfVVUlxcXFUllZKUVFRZk+nKzQuOwN2fbcr2VAzWeRdV/n7ikFB/5A+h5yukhRaUaPDwAAAEh1NiDYdBa2LRuXvCab/3u37FWzQHKsQOSump5jpNteU8QaOUlk4AEiOfkZPVQAAAAgHgSbLu6rlatk0Qt/kVFb5sh4a7m4rJ2NcrbLI1bJPiKDDhYpO1hEr/caLuLOyegxAwAAAM0RbGCs3FIjT8/7SLYtmSMT7I/kcNdn0s+q2PXdcXtFeo8U6TcmdOk7SqTHYJHiMpH8nqExPQAAAECaEWwQe0LU++SZD9bJsx+uk20bVsj+1jIZ71om+7tXyCjXOsmz61t/x7yFIsWDRAr7iXTr17Tsu/N2tz4713lyeecBAACQNAQbtGr55hp59qN1MvujDbK+ok4sCcpAa5uMdq2To3tulXG5G2RQcIN0r98o7rqtib2TecVNYadvqKXHXHqELnk9om733HlbH+Ny84kBAABgFwQb7JYWw/t0fZW88nm5/PfzTfJlefUu23R3+2X/HtWyT2GNlOXukBJXlfSxKqVHsEK6B7ZLXsM28dRtFat2q1hBfzvfdcvMzWMCjre7iLebSG5haKm3I9d1Wbjztic/1ELkyRPJyQstw7ejLy4qmgMAAGQrRwSbBx54QO644w4pLy+XcePGyX333ScHH3zwbh9HuefMWLu9Vt5duU0+WlshH62pkBVbaqTRH4zrsS4rKKW5jTI0r0YG5dTIAE+N9PXUSk9XrRRLjRTaNVIYrJb8QLXk+6sk118tOb5K8fhrU/66xJUTqgIXHXp0TJHbI+LSS06ocIJeN0u93dp6Xbpbvs/syyVi6f3u2KWOUWq+zixdu94211vZT3h9S48zrV5W0z6aX8LrGSsFAACyS8aDzZNPPinnnHOOPPTQQ3LIIYfIPffcI08//bQsXbpU+vXr1+ZjCTbOEAjasrGyTlZt3SFfb6uVLdUNsqWmQTZXhZZbqxtk244GqffFF35akiN+KZYdUmzVSJHUSoFVL4VSL0WuBil2N0iRXqwGKXTVS6FVL92sBukmdVIg9eIVn3jtRvFKo1nm2I3iCTaKx24Qt72z1DWaiQ48LQYhXddKQIpsH31/K/vYbchqdr21YzHHHN5fK8t4tjGbtXZ/W/e1sG27nr+t40nk+Vs69gSeP7Jt5IHN1sWzTfRdrT3OSuM20evaOub2vNYEton7dSRrP8l+Hc3321wr69m+He9Pou9lax8Jn1Xq3/sEt+cPiJ0z2GiYOeigg+T+++83t4PBoJSVlckvfvELufrqq9t8LMEmuzT4A1JV55fKOp+5VOmlfuf1HY0BqW3wm2VdY0B2NPqltiG0bH47mKQz0S0BE3zypFFyxSe51s7rusyx/JIjAfE0u+j6yHWz9O+8bu1c55agCWWhx4S21+fU9XpxNS3dli0evW3ZkfvcVtNSQuu0tSv8GJfYkcdGlnbUdQnEbBe+DgAAso+dpKCVrP20pOHHb0h+6VjJpESygSfZT97Y2CgffPCBzJgxI7LO5XLJpEmTZMGCBbts39DQYC7RB4/sketxS9/ueulYRTTN1w3+oNQ2BqTeFzDXNTQ1+IK7XG8MtLzeXPcHxRewxR8Iij9oi0+XejvYtL5pWRcImlap8DrdxhdeRq0L2rbZTuN/wA7dTk3nzfZqCkhNYUftDD/6VRe6bjXd3nl953rLCt8XtS5qe10fvm21ts6K3W/sflpaF7uP0PVQt8bmx6oXFb4eanuIXh+6Hf5q33W71u+P7Nccf+j9tNrYT/P7dx5X82NqfT/RxxF9/KHj2N1riT2O2Me3vE3zYw2Lfv3N75NW7ov9e3/sOn0PO7afZvuL2U/r97X2uJaeo3372f3raetxMccRNadYh/bTyutp63FtvdaW9tPScca/PrX7EYcdT/RccUAiWjvXEv2h0ep/K3bHPw/tqTOkVLJG0oPN1q1bJRAISP/+/WPW6+0vv/xyl+1nzpwpN954Y7IPA1nGsizJy3Gbi9NpCNPWJQ08GnSC0beb1mkIsqO3CUrUeg1LEhOabAkvd+5fYtaFtg9v2/S/qHWhx0WWui6e/TXdH3pdu9lf0za61uwvcn/0c+z8Hm3eGBy+GXpU9O22749+33fdV3z7Dq+I3t5O4FiaLWLes/a8hniPe+fjm92/m+eJfVQL61u4I/Ja49xJS6tb6wDQ8rYJPV2L+2713+wEXl+rx5GMfSSwbUtbp/I9SuyY4/911Pp+43/vkrKPVg8w/v3uTnv+yNXae9lWyErk/Y/ejx3PD+hdjm/3x7PLY3Z5tujtWzlOu+Vu7NZuXmvze+M5zubfo/Eco54TiQbf2OeK/xibPyaeQB/PZ2s1W7+7z7a19+fWXiMkmyQ92CRKW3amT58e02Kj3dYAJ4cwtyXidiXYHxoAAADZE2z69OkjbrdbNm3aFLNeb5eUlOyyfW5urrkAAAAAQHslfZIPr9crBxxwgMydOzeyTosH6O0JEyYk++kAAAAAIDVd0bRr2bRp0+TAAw80c9douecdO3bIj370I95yAAAAANkRbE4//XTZsmWLXHfddWaCzv3220/mzJmzS0EBAAAAAEiGlMxj0xHMYwMAAAAg0WyQ9DE2AAAAAJBuBBsAAAAAWY9gAwAAACDrEWwAAAAAZD2CDQAAAICsR7ABAAAAkPUINgAAAACyHsEGAAAAQNYj2AAAAADIeh5xGNu2I7OMAgAAAOi6qpoyQTgjZFWwqa6uNsuysrJMHwoAAAAAh2SE4uLiNrex7HjiTxoFg0HZsGGDdO/eXSzLckRK1JC1du1aKSoqyvThIAtwzoBzBnzPwGn4twnZes5oVNFQU1paKi6XK7tabPSABw0aJE6jHyjBBpwz4HsGTsK/TeCcQVf4nineTUtNGMUDAAAAAGQ9gg0AAACArEew2Y3c3Fy5/vrrzRKIB+cMEsU5A84ZpBrfM+gK54zjigcAAAAAQKJosQEAAACQ9Qg2AAAAALIewQYAAABA1iPYAAAAAMh6BBsAAAAAWY9g04YHHnhAhg4dKnl5eXLIIYfIe++9l75PBo4xc+ZMOeigg6R79+7Sr18/Oemkk2Tp0qUx29TX18tFF10kvXv3lsLCQjn11FNl06ZNMdusWbNGjjvuOCkoKDD7+dWvfiV+vz/NrwaZcNttt4llWXLppZdG1nHOoCXr16+XH/7wh+a7JD8/X/bZZx9ZtGhR5H4tZHrdddfJgAEDzP2TJk2SZcuWxexj+/btctZZZ5mZwnv06CHnn3++1NTU8IZ3QoFAQK699loZNmyYOR9GjBghN998szlPwjhnurb58+fLCSecIKWlpebfodmzZ8fcn6zzY8mSJXLkkUea38xlZWVy++23S0ZouWfs6oknnrC9Xq/98MMP25999pl9wQUX2D169LA3bdrE29XFTJ482X7kkUfsTz/91F68eLF97LHH2oMHD7Zramoi2/z0pz+1y8rK7Llz59qLFi2yDz30UPuwww6L3O/3++29997bnjRpkv3RRx/ZL774ot2nTx97xowZGXpVSJf33nvPHjp0qL3vvvvav/zlLyPrOWfQ3Pbt2+0hQ4bY5557rr1w4UJ75cqV9ssvv2wvX748ss1tt91mFxcX27Nnz7Y//vhj+7vf/a49bNgwu66uLrLNlClT7HHjxtnvvvuu/eabb9p77LGHfeaZZ/KGd0K33HKL3bt3b/v555+3V61aZT/99NN2YWGh/Yc//CGyDedM1/biiy/av/nNb+xnnnlG06797LPPxtyfjPOjsrLS7t+/v33WWWeZ30qPP/64nZ+fb//v//6vnW4Em1YcfPDB9kUXXRS5HQgE7NLSUnvmzJnp+mzgUJs3bzZfDvPmzTO3Kyoq7JycHPMPStgXX3xhtlmwYEHki8Xlctnl5eWRbWbNmmUXFRXZDQ0NGXgVSIfq6mp75MiR9iuvvGJ/+9vfjgQbzhm05KqrrrKPOOKIVt+cYDBol5SU2HfccUdknZ5Lubm55oeE+vzzz813z/vvvx/Z5qWXXrIty7LXr1/PG9/JHHfccfZ5550Xs+6UU04xPzAV5wyiNQ82yTo/HnzwQbtnz54xv2f0+2zUqFF2utEVrQWNjY3ywQcfmOa4MJfLZW4vWLAgnQ1qcKDKykqz7NWrl1nqueLz+WLOl9GjR8vgwYMj54sutUtJ//79I9tMnjxZqqqq5LPPPkv7a0B6aPdE7X4YfW4ozhm05N///rcceOCB8v3vf990Vx0/frz86U9/ity/atUqKS8vjzmfiouLTVfp6O8a7Sqi+wnT7fXfsIULF/LGdzKHHXaYzJ07V7766itz++OPP5a33npLpk6dam5zzqAtyTo/dJtvfetb4vV6Y37jaLf9b775RtLJk9ZnyxJbt241/Vajf4Qqvf3ll19m7LiQecFg0IyTOPzww2Xvvfc26/RLQf9j1v/wm58vel94m5bOp/B96HyeeOIJ+fDDD+X999/f5T7OGbRk5cqVMmvWLJk+fbr8+te/NufOJZdcYr5fpk2bFvmuaOm7JPq7RkNRNI/HY/4Qw3dN53P11VebP5DpH9Pcbrf57XLLLbeY8RCKcwZtSdb5oUsd59V8H+H7evbsKelCsAES/Av8p59+av4iBrRm7dq18stf/lJeeeUVM5ASiPcPJ/pX0VtvvdXc1hYb/b556KGHTLABmnvqqafkH//4h/zzn/+UsWPHyuLFi80f33SgOOcMuiK6orWgT58+5i8fzata6e2SkpJ0fTZwmIsvvlief/55ef3112XQoEGR9XpOaPfFioqKVs8XXbZ0PoXvQ+eiXc02b94s+++/v/nLll7mzZsn9957r7muf8ninEFzWpVor732ilk3ZswYU1Ex+ruirX+bdKnnXjStvqhVjfiu6Xy0uqa22pxxxhmmu/PZZ58tl112manmqThn0JZknR9O+o1DsGmBNvsfcMABpt9q9F/S9PaECRPS+fnAAXS8nYaaZ599Vl577bVdmlv1XMnJyYk5X7Rfqf4YCZ8vuvzkk09ivhz0r/laOrH5Dxlkv4kTJ5rPW/96Gr7oX+K1e0j4OucMmtMurs1LyevYiSFDhpjr+t2jPxKiv2u0G5L2c4/+rtE/smi4DtPvLf03TPvNo3Opra01Yx2i6R9m9fNWnDNoS7LOD91Gy0rreOPo3zijRo1Kazc0I+3lCrKo3LNWhXj00UdNRYgLL7zQlHuOrmqFruFnP/uZKYX4xhtv2Bs3boxcamtrY0r3agno1157zZR7njBhgrk0L/d8zDHHmJLRc+bMsfv27Uu55y4kuiqa4pxBS6XBPR6PKeG7bNky+x//+IddUFBg//3vf48pzar/Fj333HP2kiVL7BNPPLHF0qzjx483JaPfeustU5mPcs+d07Rp0+yBAwdGyj1rSV+dSuDKK6+MbMM507VVV1ebaSb0oj/77777bnP966+/Ttr5oZXUtNzz2Wefbco9629o/e6i3LPD3HfffebHqs5no+WftX43uh79ImjponPbhOkXwM9//nNT7lD/Yz755JNN+Im2evVqe+rUqaa2u/7Dc/nll9s+ny8DrwhOCDacM2jJf/7zH/NHEP3D2ujRo+0//vGPMfdredZrr73W/IjQbSZOnGgvXbo0Zptt27aZHx06n4mWlP/Rj35kftyg86mqqjLfK/pbJS8vzx4+fLiZsyS67C7nTNf2+uuvt/gbRkNxMs8PnQNHy9XrPjRsa2DKBEv/L71tRAAAAACQXIyxAQAAAJD1CDYAAAAAsh7BBgAAAEDWI9gAAAAAyHoEGwAAAABZj2ADAAAAIOsRbAAAAABkPYINAAAAgKxHsAEAAACQ9Qg2AAAAALIewQYAAACAZLv/B8GqP7jwW6PEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e0603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear1 parameters: 300\n",
      "Linear2 parameters: 20200\n",
      "Linear3 parameters: 20100\n",
      "Linear4 parameters: 303\n",
      "total parameters: 40903\n",
      "Model loaded from best-nonlinear-model.npz\n",
      "Test Accuracy: 0.9333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       1.00      0.93      0.97        15\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.94      0.93      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14  0  1]\n",
      " [ 1 14  0]\n",
      " [ 1  0 14]]\n"
     ]
    }
   ],
   "source": [
    "trained_model = ArtificialNeuralNetwork(2, 3)\n",
    "trained_model.load(\"best-nonlinear-model.npz\")\n",
    "\n",
    "y_pred_test = trained_model.predict(x_test)\n",
    "test_acc = np.mean(y_pred_test == y_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07bc1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0497874e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00],\n",
       "         [ 9.6008e-04,  1.0055e-02],\n",
       "         [ 1.0459e-02,  1.7284e-02],\n",
       "         [ 8.7922e-04,  3.0290e-02],\n",
       "         [ 9.9173e-03,  3.9168e-02],\n",
       "         [ 6.7728e-03,  5.0049e-02],\n",
       "         [ 1.4191e-02,  5.8921e-02],\n",
       "         [ 3.1158e-02,  6.3472e-02],\n",
       "         [-8.0324e-03,  8.0408e-02],\n",
       "         [ 4.4474e-02,  7.9287e-02],\n",
       "         [ 4.4275e-02,  9.0790e-02],\n",
       "         [ 1.3566e-02,  1.1028e-01],\n",
       "         [ 4.5101e-02,  1.1251e-01],\n",
       "         [ 8.1822e-02,  1.0270e-01],\n",
       "         [ 1.0292e-01,  9.6986e-02],\n",
       "         [ 1.0946e-01,  1.0476e-01],\n",
       "         [ 9.8855e-02,  1.2786e-01],\n",
       "         [ 1.3391e-01,  1.0749e-01],\n",
       "         [ 1.6729e-01,  7.1226e-02],\n",
       "         [ 1.7056e-01,  8.7990e-02],\n",
       "         [ 1.4095e-01,  1.4472e-01],\n",
       "         [ 1.2861e-01,  1.6868e-01],\n",
       "         [ 1.4243e-01,  1.7058e-01],\n",
       "         [ 1.9089e-01,  1.3242e-01],\n",
       "         [ 1.8765e-01,  1.5348e-01],\n",
       "         [ 1.9400e-01,  1.6166e-01],\n",
       "         [ 2.3511e-01,  1.1703e-01],\n",
       "         [ 2.3811e-01,  1.3297e-01],\n",
       "         [ 2.0683e-01,  1.9291e-01],\n",
       "         [ 2.4542e-01,  1.5993e-01],\n",
       "         [ 2.5522e-01,  1.6338e-01],\n",
       "         [ 2.8013e-01,  1.3993e-01],\n",
       "         [ 3.1688e-01,  6.3775e-02],\n",
       "         [ 3.3231e-01,  2.6103e-02],\n",
       "         [ 3.3940e-01,  5.2483e-02],\n",
       "         [ 3.2152e-01,  1.4702e-01],\n",
       "         [ 3.6289e-01, -2.3226e-02],\n",
       "         [ 3.7246e-01, -3.0879e-02],\n",
       "         [ 3.4405e-01,  1.7019e-01],\n",
       "         [ 3.9334e-01,  2.1648e-02],\n",
       "         [ 4.0241e-01, -3.6209e-02],\n",
       "         [ 4.0948e-01, -6.1943e-02],\n",
       "         [ 4.2346e-01, -2.5703e-02],\n",
       "         [ 4.1658e-01, -1.2293e-01],\n",
       "         [ 4.3901e-01, -6.9300e-02],\n",
       "         [ 4.4399e-01, -9.7380e-02],\n",
       "         [ 4.2167e-01, -1.9516e-01],\n",
       "         [ 4.6509e-01, -9.5282e-02],\n",
       "         [ 4.1177e-01, -2.5598e-01],\n",
       "         [ 4.8619e-01, -9.2720e-02],\n",
       "         [ 4.4173e-01, -2.4485e-01],\n",
       "         [ 4.1234e-01, -3.0880e-01],\n",
       "         [ 4.8400e-01, -2.0404e-01],\n",
       "         [ 4.1605e-01, -3.3691e-01],\n",
       "         [ 3.6859e-01, -4.0207e-01],\n",
       "         [ 3.1972e-01, -4.5434e-01],\n",
       "         [ 4.0142e-01, -3.9853e-01],\n",
       "         [ 4.7368e-01, -3.2730e-01],\n",
       "         [ 2.5992e-01, -5.2504e-01],\n",
       "         [ 4.4784e-01, -3.9320e-01],\n",
       "         [ 4.1302e-01, -4.4354e-01],\n",
       "         [ 3.2704e-01, -5.2221e-01],\n",
       "         [ 4.0908e-01, -4.7419e-01],\n",
       "         [ 4.1290e-01, -4.8423e-01],\n",
       "         [ 3.9660e-01, -5.1051e-01],\n",
       "         [ 2.1683e-01, -6.1973e-01],\n",
       "         [ 4.1152e-01, -5.2450e-01],\n",
       "         [ 1.9445e-01, -6.4823e-01],\n",
       "         [ 4.6064e-01, -5.0951e-01],\n",
       "         [ 1.1383e-01, -6.8761e-01],\n",
       "         [ 1.7560e-01, -6.8492e-01],\n",
       "         [ 3.0889e-01, -6.4724e-01],\n",
       "         [ 5.8309e-02, -7.2493e-01],\n",
       "         [-7.3363e-02, -7.3372e-01],\n",
       "         [ 2.8651e-01, -6.9038e-01],\n",
       "         [-6.1335e-02, -7.5509e-01],\n",
       "         [-4.2237e-03, -7.6767e-01],\n",
       "         [-1.0132e-01, -7.7115e-01],\n",
       "         [ 9.3519e-02, -7.8231e-01],\n",
       "         [ 2.4758e-01, -7.5860e-01],\n",
       "         [-8.4940e-02, -8.0360e-01],\n",
       "         [ 2.3468e-02, -8.1785e-01],\n",
       "         [-3.1037e-01, -7.6793e-01],\n",
       "         [ 1.1078e-01, -8.3103e-01],\n",
       "         [-4.0756e-01, -7.4419e-01],\n",
       "         [-2.7279e-01, -8.1410e-01],\n",
       "         [-4.6565e-01, -7.3334e-01],\n",
       "         [-2.7775e-01, -8.3374e-01],\n",
       "         [-3.1662e-01, -8.3059e-01],\n",
       "         [-2.2778e-01, -8.6965e-01],\n",
       "         [-3.2216e-01, -8.5009e-01],\n",
       "         [-2.6981e-01, -8.7870e-01],\n",
       "         [-4.2710e-01, -8.2533e-01],\n",
       "         [-5.2831e-01, -7.7675e-01],\n",
       "         [-7.6923e-01, -5.5662e-01],\n",
       "         [-4.7579e-01, -8.3334e-01],\n",
       "         [-6.3748e-01, -7.3071e-01],\n",
       "         [-8.8870e-01, -4.1258e-01],\n",
       "         [-7.0341e-01, -6.9650e-01],\n",
       "         [-9.5912e-01, -2.8300e-01],\n",
       "         [-0.0000e+00, -0.0000e+00],\n",
       "         [-8.8977e-03, -4.7813e-03],\n",
       "         [-1.8016e-02, -9.1404e-03],\n",
       "         [-2.9039e-02, -8.6618e-03],\n",
       "         [-2.9484e-02, -2.7626e-02],\n",
       "         [-4.3451e-02, -2.5744e-02],\n",
       "         [-5.6490e-02, -2.1953e-02],\n",
       "         [-6.3513e-02, -3.1074e-02],\n",
       "         [-7.6044e-02, -2.7336e-02],\n",
       "         [-8.7102e-02, -2.6033e-02],\n",
       "         [-7.2913e-02, -6.9905e-02],\n",
       "         [-1.0864e-01, -2.3297e-02],\n",
       "         [-1.2111e-01, -5.0818e-03],\n",
       "         [-1.2881e-01,  2.5534e-02],\n",
       "         [-1.2977e-01, -5.6183e-02],\n",
       "         [-1.4671e-01, -3.7850e-02],\n",
       "         [-1.6098e-01,  1.4275e-02],\n",
       "         [-1.7171e-01, -9.3530e-04],\n",
       "         [-1.7936e-01,  2.9767e-02],\n",
       "         [-1.9041e-01, -2.3987e-02],\n",
       "         [-2.0174e-01, -1.0643e-02],\n",
       "         [-2.1074e-01,  2.4165e-02],\n",
       "         [-2.1775e-01,  4.4338e-02],\n",
       "         [-2.2842e-01,  4.2385e-02],\n",
       "         [-2.4213e-01,  1.1919e-02],\n",
       "         [-2.3683e-01,  8.7635e-02],\n",
       "         [-2.5690e-01,  5.4530e-02],\n",
       "         [-2.7247e-01, -1.1789e-02],\n",
       "         [-2.3167e-01,  1.6223e-01],\n",
       "         [-2.7763e-01,  9.3444e-02],\n",
       "         [-2.9150e-01,  8.2798e-02],\n",
       "         [-2.3345e-01,  2.0869e-01],\n",
       "         [-2.4958e-01,  2.0540e-01],\n",
       "         [-1.5582e-01,  2.9467e-01],\n",
       "         [-2.8608e-01,  1.9002e-01],\n",
       "         [-2.3304e-01,  2.6585e-01],\n",
       "         [-3.3898e-01,  1.3162e-01],\n",
       "         [-2.7448e-01,  2.5365e-01],\n",
       "         [-3.1555e-01,  2.1854e-01],\n",
       "         [-3.2303e-01,  2.2548e-01],\n",
       "         [-2.9903e-01,  2.7172e-01],\n",
       "         [-1.5926e-01,  3.8229e-01],\n",
       "         [-2.1233e-01,  3.6729e-01],\n",
       "         [-2.4871e-01,  3.5609e-01],\n",
       "         [-1.9048e-01,  4.0156e-01],\n",
       "         [-1.4334e-01,  4.3135e-01],\n",
       "         [-2.1124e-01,  4.1385e-01],\n",
       "         [-5.1484e-02,  4.7195e-01],\n",
       "         [-4.7760e-02,  4.8249e-01],\n",
       "         [-2.8501e-01,  4.0465e-01],\n",
       "         [-2.1855e-01,  4.5531e-01],\n",
       "         [-5.8723e-02,  5.1179e-01],\n",
       "         [ 4.7396e-02,  5.2311e-01],\n",
       "         [-1.1282e-01,  5.2333e-01],\n",
       "         [-1.5335e-01,  5.2345e-01],\n",
       "         [-3.9476e-02,  5.5415e-01],\n",
       "         [-1.6904e-01,  5.3981e-01],\n",
       "         [ 1.7520e-01,  5.4845e-01],\n",
       "         [-1.9944e-01,  5.5087e-01],\n",
       "         [ 9.3398e-02,  5.8860e-01],\n",
       "         [-1.4445e-01,  5.8859e-01],\n",
       "         [ 1.7036e-01,  5.9214e-01],\n",
       "         [ 2.3595e-01,  5.8011e-01],\n",
       "         [ 9.9185e-02,  6.2859e-01],\n",
       "         [ 3.2948e-01,  5.5620e-01],\n",
       "         [ 1.4860e-01,  6.3953e-01],\n",
       "         [ 3.1383e-01,  5.8818e-01],\n",
       "         [ 2.2527e-01,  6.3817e-01],\n",
       "         [ 1.4988e-01,  6.7032e-01],\n",
       "         [ 5.4612e-01,  4.3304e-01],\n",
       "         [ 2.0180e-01,  6.7766e-01],\n",
       "         [ 2.5389e-01,  6.7073e-01],\n",
       "         [ 3.7952e-01,  6.2039e-01],\n",
       "         [ 5.7629e-01,  4.6001e-01],\n",
       "         [ 5.0502e-01,  5.5107e-01],\n",
       "         [ 2.1846e-01,  7.2539e-01],\n",
       "         [ 5.3379e-01,  5.5172e-01],\n",
       "         [ 6.2902e-01,  4.5746e-01],\n",
       "         [ 5.8815e-01,  5.2424e-01],\n",
       "         [ 7.1732e-01,  3.4960e-01],\n",
       "         [ 6.2954e-01,  5.0663e-01],\n",
       "         [ 6.1978e-01,  5.3413e-01],\n",
       "         [ 7.7875e-01,  2.8212e-01],\n",
       "         [ 7.5080e-01,  3.7308e-01],\n",
       "         [ 7.1377e-01,  4.5876e-01],\n",
       "         [ 8.1244e-01,  2.7769e-01],\n",
       "         [ 6.1257e-01,  6.1593e-01],\n",
       "         [ 8.5380e-01,  2.0809e-01],\n",
       "         [ 8.8446e-01,  8.8667e-02],\n",
       "         [ 8.6195e-01,  2.5539e-01],\n",
       "         [ 9.0609e-01,  7.3845e-02],\n",
       "         [ 8.7937e-01,  2.6761e-01],\n",
       "         [ 9.2914e-01, -1.6948e-02],\n",
       "         [ 9.3161e-01,  1.2071e-01],\n",
       "         [ 9.4688e-01,  7.0449e-02],\n",
       "         [ 9.3422e-01,  2.1923e-01],\n",
       "         [ 9.6027e-01,  1.3488e-01],\n",
       "         [ 9.7377e-01,  1.0851e-01],\n",
       "         [ 9.8445e-01, -1.0369e-01],\n",
       "         [ 9.5962e-01, -2.8131e-01],\n",
       "         [ 0.0000e+00, -0.0000e+00],\n",
       "         [ 9.4224e-03, -3.6400e-03],\n",
       "         [ 1.9380e-02, -5.7051e-03],\n",
       "         [ 2.9958e-02, -4.5609e-03],\n",
       "         [ 3.6487e-02, -1.7354e-02],\n",
       "         [ 4.9493e-02, -1.0059e-02],\n",
       "         [ 5.7127e-02, -2.0239e-02],\n",
       "         [ 6.9044e-02, -1.5246e-02],\n",
       "         [ 7.9739e-02, -1.3100e-02],\n",
       "         [ 8.5643e-02, -3.0491e-02],\n",
       "         [ 7.6077e-02, -6.6448e-02],\n",
       "         [ 1.0575e-01, -3.4093e-02],\n",
       "         [ 8.7824e-02, -8.3542e-02],\n",
       "         [ 9.6494e-02, -8.9062e-02],\n",
       "         [ 1.0279e-01, -9.7125e-02],\n",
       "         [ 1.0640e-01, -1.0787e-01],\n",
       "         [ 1.1424e-01, -1.1432e-01],\n",
       "         [ 1.0182e-01, -1.3827e-01],\n",
       "         [ 1.4081e-01, -1.1502e-01],\n",
       "         [ 1.3644e-01, -1.3497e-01],\n",
       "         [ 9.2028e-02, -1.7984e-01],\n",
       "         [ 1.0137e-01, -1.8633e-01],\n",
       "         [ 1.0157e-01, -1.9765e-01],\n",
       "         [ 1.3498e-01, -1.8909e-01],\n",
       "         [ 5.3309e-02, -2.3649e-01],\n",
       "         [ 9.3711e-02, -2.3449e-01],\n",
       "         [ 8.7481e-02, -2.4763e-01],\n",
       "         [ 1.0919e-02, -2.7251e-01],\n",
       "         [ 3.2995e-02, -2.8090e-01],\n",
       "         [ 7.9818e-02, -2.8185e-01],\n",
       "         [ 7.9367e-02, -2.9245e-01],\n",
       "         [ 1.2322e-02, -3.1289e-01],\n",
       "         [ 1.2733e-06, -3.2323e-01],\n",
       "         [ 1.3883e-01, -3.0305e-01],\n",
       "         [-1.6607e-02, -3.4303e-01],\n",
       "         [ 6.4690e-03, -3.5348e-01],\n",
       "         [-4.7967e-02, -3.6046e-01],\n",
       "         [-6.3134e-02, -3.6837e-01],\n",
       "         [-3.8993e-02, -3.8185e-01],\n",
       "         [-2.8746e-02, -3.9289e-01],\n",
       "         [-1.8665e-01, -3.5834e-01],\n",
       "         [-9.5927e-02, -4.0288e-01],\n",
       "         [-9.0435e-02, -4.1449e-01],\n",
       "         [-2.0448e-02, -4.3386e-01],\n",
       "         [-6.0798e-02, -4.4027e-01],\n",
       "         [-2.0171e-01, -4.0734e-01],\n",
       "         [-1.6852e-01, -4.3301e-01],\n",
       "         [-1.7689e-01, -4.4056e-01],\n",
       "         [-1.2017e-02, -4.8470e-01],\n",
       "         [-2.0005e-01, -4.5272e-01],\n",
       "         [-3.1007e-01, -3.9866e-01],\n",
       "         [-2.5674e-01, -4.4661e-01],\n",
       "         [-4.0291e-01, -3.3698e-01],\n",
       "         [-3.9551e-01, -3.6079e-01],\n",
       "         [-2.1578e-01, -5.0096e-01],\n",
       "         [-2.8422e-01, -4.7735e-01],\n",
       "         [-3.5748e-01, -4.3838e-01],\n",
       "         [-3.4696e-01, -4.5947e-01],\n",
       "         [-4.5392e-01, -3.7039e-01],\n",
       "         [-4.5549e-01, -3.8431e-01],\n",
       "         [-4.9693e-01, -3.4694e-01],\n",
       "         [-3.6777e-01, -4.9437e-01],\n",
       "         [-4.9741e-01, -3.8050e-01],\n",
       "         [-6.2562e-01, -1.1645e-01],\n",
       "         [-6.2684e-01, -1.5810e-01],\n",
       "         [-5.9781e-01, -2.7147e-01],\n",
       "         [-5.9477e-01, -3.0115e-01],\n",
       "         [-6.7425e-01, -5.8277e-02],\n",
       "         [-6.6230e-01, -1.8206e-01],\n",
       "         [-6.9697e-01, -1.3079e-03],\n",
       "         [-6.9921e-01, -1.0512e-01],\n",
       "         [-6.8089e-01,  2.2521e-01],\n",
       "         [-7.2281e-01, -8.0440e-02],\n",
       "         [-7.3736e-01, -4.6754e-03],\n",
       "         [-6.7751e-01, -3.1575e-01],\n",
       "         [-7.4278e-01,  1.4897e-01],\n",
       "         [-7.6763e-01,  8.6624e-03],\n",
       "         [-7.7775e-01, -6.1738e-03],\n",
       "         [-7.8516e-01,  6.5339e-02],\n",
       "         [-7.9423e-01, -7.7303e-02],\n",
       "         [-7.7683e-01,  2.2256e-01],\n",
       "         [-5.7345e-01,  5.8359e-01],\n",
       "         [-7.7169e-01,  3.0090e-01],\n",
       "         [-7.4569e-01,  3.8319e-01],\n",
       "         [-7.0974e-01,  4.6497e-01],\n",
       "         [-8.2092e-01,  2.5151e-01],\n",
       "         [-8.1225e-01,  3.0801e-01],\n",
       "         [-6.4810e-01,  5.9350e-01],\n",
       "         [-8.0340e-01,  3.8035e-01],\n",
       "         [-7.0861e-01,  5.5323e-01],\n",
       "         [-7.2687e-01,  5.4599e-01],\n",
       "         [-7.8002e-01,  4.8630e-01],\n",
       "         [-7.9552e-01,  4.8035e-01],\n",
       "         [-5.7702e-01,  7.4129e-01],\n",
       "         [-6.8314e-01,  6.5944e-01],\n",
       "         [-6.1748e-01,  7.3453e-01],\n",
       "         [-8.8538e-01,  3.9549e-01],\n",
       "         [-5.4608e-01,  8.1351e-01],\n",
       "         [-6.2159e-01,  7.7040e-01],\n",
       "         [-8.8751e-01,  4.6078e-01]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data_path = \"NonLinear_data.npy\"\n",
    "data = np.load(data_path, allow_pickle=True).item()\n",
    "X, labels = data[\"X\"], data[\"labels\"]\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45f7d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.010055</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.030290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.039168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_0  Feature_1  Target\n",
       "0   0.000000   0.000000       0\n",
       "1   0.000960   0.010055       0\n",
       "2   0.010459   0.017284       0\n",
       "3   0.000879   0.030290       0\n",
       "4   0.009917   0.039168       0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np = X.detach().cpu().numpy()\n",
    "\n",
    "column_names = [f\"Feature_{i}\" for i in range(X_np.shape[1])]\n",
    "\n",
    "df = pd.DataFrame(X_np, columns=column_names)\n",
    "\n",
    "df['Target'] = labels.detach().cpu().numpy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "052dbc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So dac trung: 2\n",
      "So nhan: 3\n",
      "Phan phoi nhan: [100 100 100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((300, 2), (300,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.drop(['Target'], axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "print('So dac trung:', x.shape[1])\n",
    "print('So nhan:', len(y.unique()))\n",
    "print('Phan phoi nhan:', np.bincount(y))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da8e4f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 210 samples\n",
      "  Class distribution: [70 70 70]\n",
      "Val set: 45 samples\n",
      "  Class distribution: [15 15 15]\n",
      "Test set: 45 samples\n",
      "  Class distribution: [15 15 15]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {x_train.shape[0]} samples\")\n",
    "print(f\"  Class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Val set: {x_val.shape[0]} samples\")\n",
    "print(f\"  Class distribution: {np.bincount(y_val)}\")\n",
    "print(f\"Test set: {x_test.shape[0]} samples\")\n",
    "print(f\"  Class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "279e2543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalized using StandardScaler\n",
      "Train mean: [ 9.0826129e-09 -2.4977185e-08]... (first 3 features)\n",
      "Train std: [1. 1.]... (first 3 features)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "print(\"Features normalized using StandardScaler\")\n",
    "print(f\"Train mean: {x_train_scaled.mean(axis=0)[:3]}... (first 3 features)\")\n",
    "print(f\"Train std: {x_train_scaled.std(axis=0)[:3]}... (first 3 features)\")\n",
    "\n",
    "x_train, x_val, x_test = x_train_scaled, x_val_scaled, x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfcd9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 2)\n",
      "Output shape: (210, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.10058685, -0.06348844, -0.06228777, ..., -0.12954878,\n",
       "         0.09824769, -0.04944044],\n",
       "       [ 0.00425954,  0.77222939, -1.19486787, ..., -0.57943912,\n",
       "         0.41862314, -0.64311175],\n",
       "       [-1.32908907, -0.24228387,  2.48644997, ...,  2.51256241,\n",
       "        -1.87644634,  1.5476331 ],\n",
       "       ...,\n",
       "       [-0.02619846,  0.66109675, -0.97544932, ..., -0.44366064,\n",
       "         0.3191523 , -0.52030862],\n",
       "       [ 0.06370305, -0.36057334,  0.45344202, ...,  0.15523766,\n",
       "        -0.1091248 ,  0.23369728],\n",
       "       [-1.227454  ,  0.71804591,  0.84732578, ...,  1.62286914,\n",
       "        -1.22923466,  0.65021978]], shape=(210, 100))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        self.weights = np.random.uniform(-1, 1, (input_dim, output_dim))\n",
    "        self.biases = np.zeros((output_dim,))\n",
    "\n",
    "        self.input = None\n",
    "\n",
    "        self.num_params = output_dim * input_dim + output_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return x @ self.weights + self.biases\n",
    "    \n",
    "    def backward(self, grad_output, lr):\n",
    "        grad_input = grad_output @ self.weights.T\n",
    "\n",
    "        self.lr = lr\n",
    "        m = self.input.shape[0]\n",
    "\n",
    "        d_weights = (self.input.T @ grad_output) / m\n",
    "        d_biases = np.mean(grad_output, axis=0)\n",
    "\n",
    "        self.weights -= lr * d_weights\n",
    "        self.biases -= lr * d_biases\n",
    "\n",
    "        return grad_input\n",
    "    \n",
    "linear = Linear(2, 100)\n",
    "print(x_train.shape)\n",
    "output = linear.forward(x_train)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18e74173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (210, 100)\n",
      "[[0.10058685 0.         0.         ... 0.         0.09824769 0.        ]\n",
      " [0.00425954 0.77222939 0.         ... 0.         0.41862314 0.        ]\n",
      " [0.         0.         2.48644997 ... 2.51256241 0.         1.5476331 ]\n",
      " ...\n",
      " [0.         0.66109675 0.         ... 0.         0.3191523  0.        ]\n",
      " [0.06370305 0.         0.45344202 ... 0.15523766 0.         0.23369728]\n",
      " [0.         0.71804591 0.84732578 ... 1.62286914 0.         0.65021978]]\n"
     ]
    }
   ],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output * (self.input > 0).astype(float)\n",
    "        return grad_input\n",
    "    \n",
    "relu = ReLU()\n",
    "output = relu.forward(output)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "630bd641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (210, 100)\n"
     ]
    }
   ],
   "source": [
    "class Softmax():\n",
    "    def forward(self, x):\n",
    "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, y, y_pred):\n",
    "        m = y.shape[0]\n",
    "        d_scores = y_pred.copy()\n",
    "        d_scores[np.arange(m), y] -= 1\n",
    "        d_scores /= m\n",
    "        return d_scores\n",
    "\n",
    "softmax = Softmax()\n",
    "output = softmax.forward(output)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27e004ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, y_pred):\n",
    "    n = y.shape[0]\n",
    "    n_classes = y_pred.shape[1]\n",
    "    y_onehot = np.zeros((n, n_classes))\n",
    "    y_onehot[np.arange(n), y] = 1\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = -np.mean(np.sum(y_onehot * np.log(y_pred), axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "752596cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear1 parameters: 300\n",
      "Linear2 parameters: 20200\n",
      "Linear3 parameters: 20100\n",
      "Linear4 parameters: 303\n",
      "total parameters: 40903\n",
      "Output shape: (210, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.99563703e-001, 4.28496554e-004, 7.79996803e-006],\n",
       "       [1.00000000e+000, 1.88784645e-041, 1.13422200e-043],\n",
       "       [1.00000000e+000, 6.21112229e-141, 2.16228333e-116],\n",
       "       [1.00000000e+000, 2.41563082e-023, 2.89233171e-040],\n",
       "       [1.00000000e+000, 1.31218412e-059, 5.25865312e-090],\n",
       "       [1.00000000e+000, 1.86760768e-021, 3.14948014e-027],\n",
       "       [1.00000000e+000, 1.10798194e-087, 1.92572712e-052],\n",
       "       [1.00000000e+000, 4.51228502e-010, 1.78144348e-011],\n",
       "       [1.00000000e+000, 1.20868658e-022, 3.74595257e-041],\n",
       "       [1.00000000e+000, 5.23490252e-054, 5.08504757e-063],\n",
       "       [1.00000000e+000, 1.32184633e-025, 1.16405177e-017],\n",
       "       [1.00000000e+000, 2.24633699e-018, 5.21255203e-013],\n",
       "       [1.00000000e+000, 4.53961229e-026, 4.98247594e-045],\n",
       "       [1.00000000e+000, 1.12406135e-014, 9.13110626e-018],\n",
       "       [1.00000000e+000, 3.70232882e-012, 7.09858644e-014],\n",
       "       [1.00000000e+000, 2.56493258e-069, 1.59643806e-090],\n",
       "       [1.00000000e+000, 5.30169545e-023, 9.97921227e-034],\n",
       "       [1.00000000e+000, 3.45435283e-056, 7.09641415e-094],\n",
       "       [1.00000000e+000, 9.41000902e-073, 2.34764797e-050],\n",
       "       [1.00000000e+000, 1.68009517e-081, 2.53498738e-044],\n",
       "       [9.99966578e-001, 3.34094483e-005, 1.26699637e-008],\n",
       "       [1.00000000e+000, 1.53817836e-038, 4.71079770e-071],\n",
       "       [9.99999688e-001, 3.12036140e-007, 3.28939554e-010],\n",
       "       [1.00000000e+000, 2.53643453e-074, 1.08896436e-074],\n",
       "       [1.00000000e+000, 8.67969109e-057, 6.10669579e-069],\n",
       "       [1.00000000e+000, 2.31861351e-057, 3.74424124e-035],\n",
       "       [1.00000000e+000, 9.70939698e-067, 3.91104129e-056],\n",
       "       [1.00000000e+000, 1.74243110e-024, 3.06589148e-025],\n",
       "       [1.00000000e+000, 7.74984746e-016, 2.84988417e-018],\n",
       "       [1.00000000e+000, 7.61586554e-023, 2.50745066e-039],\n",
       "       [1.00000000e+000, 2.95794266e-058, 3.63046375e-041],\n",
       "       [1.00000000e+000, 5.98129868e-069, 3.62341658e-086],\n",
       "       [1.00000000e+000, 2.18857434e-085, 1.35062532e-055],\n",
       "       [1.00000000e+000, 2.99089073e-016, 1.66241484e-030],\n",
       "       [7.25667715e-001, 1.99164256e-001, 7.51680283e-002],\n",
       "       [1.00000000e+000, 2.12760747e-027, 4.42179629e-034],\n",
       "       [1.00000000e+000, 1.49208471e-030, 7.81306174e-017],\n",
       "       [1.00000000e+000, 1.36582062e-081, 4.03774006e-051],\n",
       "       [1.00000000e+000, 1.27390617e-014, 3.54216274e-012],\n",
       "       [1.00000000e+000, 9.87166607e-029, 6.69817004e-041],\n",
       "       [1.00000000e+000, 3.31281704e-040, 5.48130264e-077],\n",
       "       [1.00000000e+000, 2.84957736e-055, 4.55961447e-067],\n",
       "       [1.00000000e+000, 5.07465612e-030, 3.10428350e-041],\n",
       "       [1.00000000e+000, 1.02842127e-057, 4.40740194e-070],\n",
       "       [1.00000000e+000, 4.54592332e-025, 6.35953906e-031],\n",
       "       [1.00000000e+000, 6.71329135e-013, 1.50862277e-015],\n",
       "       [1.00000000e+000, 6.58105860e-027, 2.06279639e-036],\n",
       "       [9.90098765e-001, 6.76790791e-003, 3.13332748e-003],\n",
       "       [1.00000000e+000, 1.46348515e-074, 2.81158985e-062],\n",
       "       [1.00000000e+000, 1.00207701e-014, 2.52427379e-010],\n",
       "       [1.00000000e+000, 7.57189080e-065, 2.80228836e-093],\n",
       "       [1.00000000e+000, 4.58839778e-022, 1.17764315e-035],\n",
       "       [1.00000000e+000, 1.15094953e-050, 1.16127579e-028],\n",
       "       [9.21796212e-001, 1.60559312e-002, 6.21478572e-002],\n",
       "       [1.00000000e+000, 1.01969770e-025, 1.05439130e-030],\n",
       "       [1.00000000e+000, 2.48614698e-074, 5.25505950e-052],\n",
       "       [1.00000000e+000, 2.26564731e-074, 7.83976552e-045],\n",
       "       [1.00000000e+000, 4.95915385e-135, 7.57678348e-088],\n",
       "       [1.00000000e+000, 1.45053051e-059, 8.59107335e-033],\n",
       "       [1.00000000e+000, 1.52043900e-060, 1.37448965e-069],\n",
       "       [1.00000000e+000, 5.80933156e-071, 4.36607517e-074],\n",
       "       [1.00000000e+000, 1.65114619e-070, 1.86534631e-047],\n",
       "       [1.00000000e+000, 4.47487439e-048, 3.14711491e-086],\n",
       "       [1.00000000e+000, 4.82459425e-019, 5.34882562e-034],\n",
       "       [1.00000000e+000, 9.27175384e-065, 3.94023324e-075],\n",
       "       [1.00000000e+000, 4.15828266e-013, 3.46138901e-015],\n",
       "       [1.00000000e+000, 2.74036104e-018, 4.79365712e-022],\n",
       "       [9.99999997e-001, 4.43961865e-014, 3.12803533e-009],\n",
       "       [1.00000000e+000, 2.35017305e-028, 1.14721837e-015],\n",
       "       [1.00000000e+000, 1.54505019e-022, 6.08596442e-036],\n",
       "       [1.00000000e+000, 4.13554820e-042, 2.45392103e-045],\n",
       "       [1.00000000e+000, 6.86172086e-014, 1.00098797e-011],\n",
       "       [1.00000000e+000, 2.34436027e-064, 9.05972822e-066],\n",
       "       [9.97028401e-001, 2.54602201e-003, 4.25576560e-004],\n",
       "       [1.00000000e+000, 9.52531000e-040, 2.62686444e-034],\n",
       "       [1.00000000e+000, 2.92861826e-070, 1.69741575e-093],\n",
       "       [1.00000000e+000, 2.68378634e-021, 9.73615869e-038],\n",
       "       [1.00000000e+000, 2.14828077e-061, 1.65700809e-037],\n",
       "       [1.00000000e+000, 3.53500948e-071, 2.10515699e-084],\n",
       "       [1.00000000e+000, 4.10282858e-066, 1.01455436e-088],\n",
       "       [1.00000000e+000, 6.55731942e-043, 9.53437492e-024],\n",
       "       [1.00000000e+000, 1.54486138e-030, 6.59673228e-023],\n",
       "       [1.00000000e+000, 2.43918037e-050, 2.10150561e-042],\n",
       "       [1.00000000e+000, 6.06789261e-048, 2.62716772e-048],\n",
       "       [1.00000000e+000, 7.29085313e-085, 2.37255039e-046],\n",
       "       [1.00000000e+000, 3.59278721e-028, 1.36151175e-026],\n",
       "       [9.99999999e-001, 9.51057398e-016, 5.68012016e-010],\n",
       "       [1.00000000e+000, 1.16109571e-022, 3.87207925e-025],\n",
       "       [1.00000000e+000, 1.10206577e-084, 1.54667629e-095],\n",
       "       [1.00000000e+000, 5.45082495e-063, 2.01761107e-073],\n",
       "       [1.00000000e+000, 5.21112379e-071, 2.36966098e-092],\n",
       "       [1.00000000e+000, 2.40314266e-055, 8.90356848e-067],\n",
       "       [9.99998967e-001, 1.03120203e-006, 2.19256428e-009],\n",
       "       [1.00000000e+000, 4.30911572e-057, 2.71908965e-068],\n",
       "       [1.00000000e+000, 5.01458727e-018, 4.34597308e-011],\n",
       "       [1.00000000e+000, 7.89611139e-020, 3.08821184e-012],\n",
       "       [9.99999921e-001, 7.88208526e-008, 7.62808405e-011],\n",
       "       [9.99934103e-001, 2.00569723e-007, 6.56965415e-005],\n",
       "       [1.00000000e+000, 5.44259676e-056, 2.97493893e-090],\n",
       "       [1.00000000e+000, 6.93969606e-020, 1.83561916e-013],\n",
       "       [9.99999892e-001, 3.90670878e-009, 1.04122849e-007],\n",
       "       [9.99558085e-001, 4.39592454e-004, 2.32222761e-006],\n",
       "       [1.00000000e+000, 3.11198283e-096, 2.57945864e-052],\n",
       "       [1.00000000e+000, 3.09820995e-019, 3.61868211e-022],\n",
       "       [9.99999632e-001, 3.40621271e-007, 2.73251133e-008],\n",
       "       [9.99989780e-001, 1.02088956e-005, 1.09271790e-008],\n",
       "       [1.00000000e+000, 3.70433522e-039, 6.41918132e-047],\n",
       "       [1.00000000e+000, 7.51459960e-043, 1.96763645e-037],\n",
       "       [1.00000000e+000, 8.60353525e-025, 1.04148016e-041],\n",
       "       [1.00000000e+000, 2.86426782e-023, 6.89457395e-015],\n",
       "       [1.00000000e+000, 1.06284314e-090, 5.13559423e-066],\n",
       "       [1.00000000e+000, 2.11224454e-064, 5.65595742e-097],\n",
       "       [1.00000000e+000, 1.97767607e-018, 6.64414276e-015],\n",
       "       [1.00000000e+000, 3.69545017e-083, 1.71065275e-050],\n",
       "       [1.00000000e+000, 1.05139664e-076, 1.30740953e-055],\n",
       "       [1.00000000e+000, 7.17540047e-041, 3.60952489e-047],\n",
       "       [1.00000000e+000, 2.44857361e-064, 5.59400608e-089],\n",
       "       [1.00000000e+000, 1.00361490e-096, 1.96789407e-058],\n",
       "       [1.00000000e+000, 1.59859738e-046, 5.24262239e-084],\n",
       "       [1.00000000e+000, 8.83039710e-144, 4.70184365e-098],\n",
       "       [1.00000000e+000, 5.78231445e-083, 1.86728849e-100],\n",
       "       [1.00000000e+000, 1.41093246e-025, 3.14437862e-035],\n",
       "       [9.99566722e-001, 1.78872589e-005, 4.15391139e-004],\n",
       "       [1.00000000e+000, 1.97705900e-076, 6.23681101e-106],\n",
       "       [1.00000000e+000, 2.25570373e-090, 1.09481821e-054],\n",
       "       [1.00000000e+000, 2.80662261e-013, 4.40287889e-011],\n",
       "       [1.00000000e+000, 8.23253766e-060, 2.46027684e-072],\n",
       "       [1.00000000e+000, 3.08385850e-038, 5.43521283e-021],\n",
       "       [1.00000000e+000, 5.81566759e-085, 1.22252513e-053],\n",
       "       [1.00000000e+000, 6.38520602e-031, 8.95947059e-038],\n",
       "       [1.00000000e+000, 4.07774078e-010, 4.88943408e-011],\n",
       "       [1.00000000e+000, 1.83558310e-068, 1.02175529e-052],\n",
       "       [9.93936250e-001, 5.86627154e-003, 1.97478089e-004],\n",
       "       [1.00000000e+000, 2.83432213e-051, 3.40080577e-033],\n",
       "       [1.00000000e+000, 2.04504592e-066, 1.00188603e-081],\n",
       "       [1.00000000e+000, 1.47228226e-043, 1.69947098e-042],\n",
       "       [1.00000000e+000, 2.28669312e-052, 1.33452640e-063],\n",
       "       [1.00000000e+000, 2.30000494e-044, 4.60882199e-080],\n",
       "       [1.00000000e+000, 6.54901051e-026, 9.80977954e-043],\n",
       "       [1.00000000e+000, 1.45173210e-026, 8.41471012e-030],\n",
       "       [9.99990472e-001, 3.96740522e-008, 9.48786248e-006],\n",
       "       [9.99998052e-001, 7.64545859e-010, 1.94732957e-006],\n",
       "       [1.00000000e+000, 8.45198027e-075, 5.47259577e-073],\n",
       "       [1.00000000e+000, 6.64429245e-137, 1.55149821e-084],\n",
       "       [1.00000000e+000, 1.66543881e-098, 2.20320860e-055],\n",
       "       [1.00000000e+000, 1.60505091e-016, 3.12638262e-011],\n",
       "       [9.99996776e-001, 3.22390955e-006, 2.03089837e-010],\n",
       "       [1.00000000e+000, 1.10434594e-017, 9.78970251e-013],\n",
       "       [1.00000000e+000, 1.67994458e-029, 3.06128959e-034],\n",
       "       [9.99999831e-001, 7.93128626e-012, 1.69164138e-007],\n",
       "       [1.00000000e+000, 1.24588238e-044, 8.39281992e-040],\n",
       "       [1.00000000e+000, 6.39791659e-054, 6.31030931e-087],\n",
       "       [1.00000000e+000, 5.08458932e-064, 1.50125237e-072],\n",
       "       [1.00000000e+000, 7.88081389e-017, 5.79922534e-029],\n",
       "       [1.00000000e+000, 2.25126857e-016, 5.73646697e-029],\n",
       "       [1.00000000e+000, 6.44947797e-061, 2.41700792e-035],\n",
       "       [1.00000000e+000, 1.37432003e-070, 2.88605027e-059],\n",
       "       [1.00000000e+000, 1.49438966e-024, 1.02621895e-042],\n",
       "       [1.00000000e+000, 2.91630618e-014, 8.38373782e-017],\n",
       "       [1.00000000e+000, 1.99455160e-072, 6.54786619e-090],\n",
       "       [1.00000000e+000, 3.94870726e-081, 4.75221095e-058],\n",
       "       [9.99942924e-001, 5.69426631e-005, 1.33434206e-007],\n",
       "       [1.00000000e+000, 1.66518341e-038, 2.94306097e-040],\n",
       "       [1.00000000e+000, 6.00604679e-032, 1.78929018e-036],\n",
       "       [1.00000000e+000, 2.06920765e-066, 1.81393151e-037],\n",
       "       [1.00000000e+000, 6.94693675e-107, 1.62049866e-061],\n",
       "       [9.99999997e-001, 2.89508436e-009, 4.03055512e-011],\n",
       "       [1.00000000e+000, 3.80239675e-023, 3.23362598e-040],\n",
       "       [1.00000000e+000, 8.73932407e-060, 1.42137515e-066],\n",
       "       [1.00000000e+000, 4.65658843e-080, 8.64576851e-080],\n",
       "       [1.00000000e+000, 3.92672401e-080, 4.97351092e-065],\n",
       "       [1.00000000e+000, 7.86985645e-045, 1.24805344e-079],\n",
       "       [1.00000000e+000, 3.59096856e-059, 5.62342359e-088],\n",
       "       [1.00000000e+000, 3.39581540e-054, 7.37071714e-061],\n",
       "       [1.00000000e+000, 6.74655789e-068, 1.61166857e-078],\n",
       "       [1.00000000e+000, 1.50023480e-025, 2.03726225e-041],\n",
       "       [1.00000000e+000, 1.46313164e-039, 1.05685955e-041],\n",
       "       [1.00000000e+000, 4.82144624e-017, 9.38001546e-020],\n",
       "       [1.00000000e+000, 3.21193333e-080, 6.61551513e-056],\n",
       "       [1.00000000e+000, 2.85247828e-026, 1.30477056e-039],\n",
       "       [1.00000000e+000, 8.28690729e-068, 3.32071011e-085],\n",
       "       [1.00000000e+000, 1.75485567e-017, 2.15908686e-018],\n",
       "       [1.00000000e+000, 1.13883816e-050, 3.65535275e-028],\n",
       "       [1.00000000e+000, 1.71650900e-022, 7.60025503e-018],\n",
       "       [1.00000000e+000, 2.32875170e-093, 7.22810118e-051],\n",
       "       [1.00000000e+000, 4.68707528e-036, 9.78529112e-042],\n",
       "       [1.00000000e+000, 4.79768420e-128, 3.53905998e-119],\n",
       "       [1.00000000e+000, 3.84311302e-023, 1.97869074e-040],\n",
       "       [1.00000000e+000, 2.43113914e-062, 1.31325457e-037],\n",
       "       [1.00000000e+000, 2.99700386e-026, 3.72904993e-021],\n",
       "       [1.00000000e+000, 2.75479803e-031, 1.94883105e-040],\n",
       "       [1.00000000e+000, 4.34362461e-022, 2.15329839e-014],\n",
       "       [1.00000000e+000, 1.63077471e-078, 1.95944060e-062],\n",
       "       [1.00000000e+000, 4.60709569e-121, 1.51948229e-127],\n",
       "       [1.00000000e+000, 3.27812268e-015, 1.67720642e-017],\n",
       "       [1.00000000e+000, 2.19768346e-079, 3.95950034e-043],\n",
       "       [1.00000000e+000, 2.35835932e-034, 1.14051670e-032],\n",
       "       [1.00000000e+000, 1.20807107e-058, 7.08540975e-069],\n",
       "       [1.00000000e+000, 6.05893015e-024, 3.24605495e-039],\n",
       "       [1.00000000e+000, 1.46105737e-108, 2.83623550e-061],\n",
       "       [1.00000000e+000, 2.81714536e-049, 1.51363101e-028],\n",
       "       [1.00000000e+000, 4.28183467e-025, 2.10204711e-020],\n",
       "       [1.00000000e+000, 4.61292740e-075, 1.53813015e-050],\n",
       "       [1.00000000e+000, 4.58871238e-047, 3.52594165e-032],\n",
       "       [1.00000000e+000, 7.48438477e-060, 1.65206054e-047],\n",
       "       [1.00000000e+000, 1.41727424e-140, 4.48749262e-096],\n",
       "       [9.99627422e-001, 2.73339323e-004, 9.92383594e-005],\n",
       "       [1.00000000e+000, 1.53255519e-033, 8.14218994e-037],\n",
       "       [1.00000000e+000, 3.68022253e-017, 6.68440972e-013],\n",
       "       [1.00000000e+000, 1.99805246e-050, 9.30849153e-058]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        self.linear1 = Linear(input_dim, 100)\n",
    "        print(f\"Linear1 parameters: {self.linear1.num_params}\")\n",
    "        self.relu1 = ReLU()\n",
    "\n",
    "        self.linear2 = Linear(100, 200)\n",
    "        print(f\"Linear2 parameters: {self.linear2.num_params}\")\n",
    "        self.relu2 = ReLU()\n",
    "\n",
    "        self.linear3 = Linear(200, 100)\n",
    "        print(f\"Linear3 parameters: {self.linear3.num_params}\")\n",
    "        self.relu3 = ReLU()\n",
    "\n",
    "        self.linear4 = Linear(100, output_dim)\n",
    "        print(f\"Linear4 parameters: {self.linear4.num_params}\")\n",
    "        self.softmax = Softmax()\n",
    "        print(f\"total parameters: {self.linear1.num_params + self.linear2.num_params + self.linear3.num_params + self.linear4.num_params}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1.forward(x)\n",
    "        out = self.relu1.forward(out)\n",
    "        out = self.linear2.forward(out)\n",
    "        out = self.relu2.forward(out)\n",
    "        out = self.linear3.forward(out)\n",
    "        out = self.relu3.forward(out)\n",
    "        out = self.linear4.forward(out)\n",
    "        out = self.softmax.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, y, y_pred, lr):\n",
    "        grad_softmax = softmax.backward(y, y_pred)\n",
    "        grad = self.linear4.backward(grad_softmax, lr)\n",
    "        grad = self.relu3.backward(grad)\n",
    "        grad = self.linear3.backward(grad, lr)\n",
    "        grad = self.relu2.backward(grad)\n",
    "        grad = self.linear2.backward(grad, lr)\n",
    "        grad = self.relu1.backward(grad)\n",
    "        grad = self.linear1.backward(grad, lr)\n",
    "        return grad\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        np.savez(\n",
    "            filepath,\n",
    "            linear1_weights=self.linear1.weights,\n",
    "            linear1_biases=self.linear1.biases,\n",
    "            linear2_weights=self.linear2.weights,\n",
    "            linear2_biases=self.linear2.biases,\n",
    "            linear3_weights=self.linear3.weights,\n",
    "            linear3_biases=self.linear3.biases,\n",
    "            linear4_weights=self.linear4.weights,\n",
    "            linear4_biases=self.linear4.biases,\n",
    "        )\n",
    "\n",
    "    def load(self, filepath):\n",
    "        data = np.load(filepath)\n",
    "        self.linear1.weights = data['linear1_weights']\n",
    "        self.linear1.biases = data['linear1_biases']\n",
    "        self.linear2.weights = data['linear2_weights']\n",
    "        self.linear2.biases = data['linear2_biases']\n",
    "        self.linear3.weights = data['linear3_weights']\n",
    "        self.linear3.biases = data['linear3_biases']\n",
    "        self.linear4.weights = data['linear4_weights']\n",
    "        self.linear4.biases = data['linear4_biases']\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "    \n",
    "    def fit(self, x_train, y_train, x_val, y_val, epochs: int, lr: float):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(x_train)\n",
    "            self.backward(y_train, y_pred, lr)\n",
    "\n",
    "            train_loss = cross_entropy(y_train, y_pred)\n",
    "            train_losses.append(train_loss)\n",
    "            train_acc = np.mean(np.argmax(y_pred, axis=1) == y_train)\n",
    "            self.save(\"last-nonlinear-model.npz\")\n",
    "\n",
    "            y_pred_val = self.forward(x_val)\n",
    "            val_loss = cross_entropy(y_val, y_pred_val)\n",
    "            if val_losses and val_loss < val_losses[-1]:\n",
    "                self.save(\"best-nonlinear-model.npz\")\n",
    "            val_losses.append(val_loss)\n",
    "            val_acc = np.mean(np.argmax(y_pred_val, axis=1) == y_val)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = self.forward(x)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "nn = ArtificialNeuralNetwork(2, 3)\n",
    "output = nn.forward(x_train)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6a569b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 20.9879, Train Acc: 0.3333, Val Loss: 18.1464, Val Acc: 0.3333\n",
      "Epoch 10/1000, Train Loss: 9.4699, Train Acc: 0.4333, Val Loss: 8.3610, Val Acc: 0.4222\n",
      "Epoch 20/1000, Train Loss: 5.4718, Train Acc: 0.4667, Val Loss: 4.8309, Val Acc: 0.4667\n",
      "Epoch 30/1000, Train Loss: 2.8478, Train Acc: 0.5571, Val Loss: 2.5543, Val Acc: 0.5556\n",
      "Epoch 40/1000, Train Loss: 1.6182, Train Acc: 0.6571, Val Loss: 1.5386, Val Acc: 0.6444\n",
      "Epoch 50/1000, Train Loss: 1.0505, Train Acc: 0.7429, Val Loss: 0.9983, Val Acc: 0.6889\n",
      "Epoch 60/1000, Train Loss: 0.7107, Train Acc: 0.8143, Val Loss: 0.6881, Val Acc: 0.7556\n",
      "Epoch 70/1000, Train Loss: 0.4883, Train Acc: 0.8714, Val Loss: 0.5226, Val Acc: 0.8222\n",
      "Epoch 80/1000, Train Loss: 0.3333, Train Acc: 0.9000, Val Loss: 0.4237, Val Acc: 0.8444\n",
      "Epoch 90/1000, Train Loss: 0.2320, Train Acc: 0.9238, Val Loss: 0.3569, Val Acc: 0.8444\n",
      "Epoch 100/1000, Train Loss: 0.1863, Train Acc: 0.9429, Val Loss: 0.3150, Val Acc: 0.8889\n",
      "Epoch 110/1000, Train Loss: 0.1570, Train Acc: 0.9571, Val Loss: 0.2790, Val Acc: 0.9111\n",
      "Epoch 120/1000, Train Loss: 0.1350, Train Acc: 0.9714, Val Loss: 0.2483, Val Acc: 0.9111\n",
      "Epoch 130/1000, Train Loss: 0.1164, Train Acc: 0.9714, Val Loss: 0.2220, Val Acc: 0.9111\n",
      "Epoch 140/1000, Train Loss: 0.1010, Train Acc: 0.9714, Val Loss: 0.2008, Val Acc: 0.9333\n",
      "Epoch 150/1000, Train Loss: 0.0885, Train Acc: 0.9810, Val Loss: 0.1842, Val Acc: 0.9333\n",
      "Epoch 160/1000, Train Loss: 0.0781, Train Acc: 0.9810, Val Loss: 0.1704, Val Acc: 0.9333\n",
      "Epoch 170/1000, Train Loss: 0.0697, Train Acc: 0.9810, Val Loss: 0.1594, Val Acc: 0.9333\n",
      "Epoch 180/1000, Train Loss: 0.0629, Train Acc: 0.9857, Val Loss: 0.1504, Val Acc: 0.9333\n",
      "Epoch 190/1000, Train Loss: 0.0573, Train Acc: 0.9905, Val Loss: 0.1428, Val Acc: 0.9333\n",
      "Epoch 200/1000, Train Loss: 0.0530, Train Acc: 0.9905, Val Loss: 0.1370, Val Acc: 0.9333\n",
      "Epoch 210/1000, Train Loss: 0.0494, Train Acc: 0.9952, Val Loss: 0.1316, Val Acc: 0.9333\n",
      "Epoch 220/1000, Train Loss: 0.0464, Train Acc: 1.0000, Val Loss: 0.1267, Val Acc: 0.9333\n",
      "Epoch 230/1000, Train Loss: 0.0438, Train Acc: 1.0000, Val Loss: 0.1223, Val Acc: 0.9333\n",
      "Epoch 240/1000, Train Loss: 0.0414, Train Acc: 1.0000, Val Loss: 0.1181, Val Acc: 0.9556\n",
      "Epoch 250/1000, Train Loss: 0.0393, Train Acc: 1.0000, Val Loss: 0.1142, Val Acc: 0.9778\n",
      "Epoch 260/1000, Train Loss: 0.0375, Train Acc: 1.0000, Val Loss: 0.1106, Val Acc: 0.9778\n",
      "Epoch 270/1000, Train Loss: 0.0358, Train Acc: 1.0000, Val Loss: 0.1073, Val Acc: 0.9778\n",
      "Epoch 280/1000, Train Loss: 0.0343, Train Acc: 1.0000, Val Loss: 0.1043, Val Acc: 0.9778\n",
      "Epoch 290/1000, Train Loss: 0.0329, Train Acc: 1.0000, Val Loss: 0.1015, Val Acc: 0.9778\n",
      "Epoch 300/1000, Train Loss: 0.0317, Train Acc: 1.0000, Val Loss: 0.0988, Val Acc: 0.9778\n",
      "Epoch 310/1000, Train Loss: 0.0305, Train Acc: 1.0000, Val Loss: 0.0964, Val Acc: 0.9778\n",
      "Epoch 320/1000, Train Loss: 0.0295, Train Acc: 1.0000, Val Loss: 0.0942, Val Acc: 0.9778\n",
      "Epoch 330/1000, Train Loss: 0.0286, Train Acc: 1.0000, Val Loss: 0.0921, Val Acc: 0.9778\n",
      "Epoch 340/1000, Train Loss: 0.0277, Train Acc: 1.0000, Val Loss: 0.0902, Val Acc: 0.9778\n",
      "Epoch 350/1000, Train Loss: 0.0268, Train Acc: 1.0000, Val Loss: 0.0883, Val Acc: 0.9778\n",
      "Epoch 360/1000, Train Loss: 0.0261, Train Acc: 1.0000, Val Loss: 0.0866, Val Acc: 0.9778\n",
      "Epoch 370/1000, Train Loss: 0.0253, Train Acc: 1.0000, Val Loss: 0.0850, Val Acc: 0.9778\n",
      "Epoch 380/1000, Train Loss: 0.0247, Train Acc: 1.0000, Val Loss: 0.0835, Val Acc: 0.9778\n",
      "Epoch 390/1000, Train Loss: 0.0240, Train Acc: 1.0000, Val Loss: 0.0821, Val Acc: 0.9778\n",
      "Epoch 400/1000, Train Loss: 0.0234, Train Acc: 1.0000, Val Loss: 0.0807, Val Acc: 0.9778\n",
      "Epoch 410/1000, Train Loss: 0.0228, Train Acc: 1.0000, Val Loss: 0.0794, Val Acc: 0.9778\n",
      "Epoch 420/1000, Train Loss: 0.0223, Train Acc: 1.0000, Val Loss: 0.0782, Val Acc: 0.9778\n",
      "Epoch 430/1000, Train Loss: 0.0218, Train Acc: 1.0000, Val Loss: 0.0770, Val Acc: 0.9778\n",
      "Epoch 440/1000, Train Loss: 0.0213, Train Acc: 1.0000, Val Loss: 0.0759, Val Acc: 0.9778\n",
      "Epoch 450/1000, Train Loss: 0.0208, Train Acc: 1.0000, Val Loss: 0.0749, Val Acc: 0.9778\n",
      "Epoch 460/1000, Train Loss: 0.0204, Train Acc: 1.0000, Val Loss: 0.0739, Val Acc: 0.9778\n",
      "Epoch 470/1000, Train Loss: 0.0200, Train Acc: 1.0000, Val Loss: 0.0730, Val Acc: 0.9778\n",
      "Epoch 480/1000, Train Loss: 0.0196, Train Acc: 1.0000, Val Loss: 0.0721, Val Acc: 0.9778\n",
      "Epoch 490/1000, Train Loss: 0.0193, Train Acc: 1.0000, Val Loss: 0.0712, Val Acc: 0.9778\n",
      "Epoch 500/1000, Train Loss: 0.0189, Train Acc: 1.0000, Val Loss: 0.0704, Val Acc: 0.9778\n",
      "Epoch 510/1000, Train Loss: 0.0186, Train Acc: 1.0000, Val Loss: 0.0696, Val Acc: 0.9778\n",
      "Epoch 520/1000, Train Loss: 0.0183, Train Acc: 1.0000, Val Loss: 0.0688, Val Acc: 0.9778\n",
      "Epoch 530/1000, Train Loss: 0.0180, Train Acc: 1.0000, Val Loss: 0.0681, Val Acc: 0.9778\n",
      "Epoch 540/1000, Train Loss: 0.0176, Train Acc: 1.0000, Val Loss: 0.0673, Val Acc: 0.9778\n",
      "Epoch 550/1000, Train Loss: 0.0173, Train Acc: 1.0000, Val Loss: 0.0667, Val Acc: 0.9778\n",
      "Epoch 560/1000, Train Loss: 0.0170, Train Acc: 1.0000, Val Loss: 0.0660, Val Acc: 0.9778\n",
      "Epoch 570/1000, Train Loss: 0.0168, Train Acc: 1.0000, Val Loss: 0.0654, Val Acc: 0.9778\n",
      "Epoch 580/1000, Train Loss: 0.0165, Train Acc: 1.0000, Val Loss: 0.0648, Val Acc: 0.9778\n",
      "Epoch 590/1000, Train Loss: 0.0162, Train Acc: 1.0000, Val Loss: 0.0643, Val Acc: 0.9778\n",
      "Epoch 600/1000, Train Loss: 0.0160, Train Acc: 1.0000, Val Loss: 0.0637, Val Acc: 0.9778\n",
      "Epoch 610/1000, Train Loss: 0.0157, Train Acc: 1.0000, Val Loss: 0.0633, Val Acc: 0.9778\n",
      "Epoch 620/1000, Train Loss: 0.0155, Train Acc: 1.0000, Val Loss: 0.0628, Val Acc: 0.9556\n",
      "Epoch 630/1000, Train Loss: 0.0152, Train Acc: 1.0000, Val Loss: 0.0624, Val Acc: 0.9556\n",
      "Epoch 640/1000, Train Loss: 0.0150, Train Acc: 1.0000, Val Loss: 0.0620, Val Acc: 0.9556\n",
      "Epoch 650/1000, Train Loss: 0.0148, Train Acc: 1.0000, Val Loss: 0.0616, Val Acc: 0.9556\n",
      "Epoch 660/1000, Train Loss: 0.0146, Train Acc: 1.0000, Val Loss: 0.0612, Val Acc: 0.9556\n",
      "Epoch 670/1000, Train Loss: 0.0143, Train Acc: 1.0000, Val Loss: 0.0608, Val Acc: 0.9556\n",
      "Epoch 680/1000, Train Loss: 0.0141, Train Acc: 1.0000, Val Loss: 0.0604, Val Acc: 0.9556\n",
      "Epoch 690/1000, Train Loss: 0.0139, Train Acc: 1.0000, Val Loss: 0.0601, Val Acc: 0.9556\n",
      "Epoch 700/1000, Train Loss: 0.0138, Train Acc: 1.0000, Val Loss: 0.0597, Val Acc: 0.9556\n",
      "Epoch 710/1000, Train Loss: 0.0136, Train Acc: 1.0000, Val Loss: 0.0594, Val Acc: 0.9556\n",
      "Epoch 720/1000, Train Loss: 0.0134, Train Acc: 1.0000, Val Loss: 0.0591, Val Acc: 0.9556\n",
      "Epoch 730/1000, Train Loss: 0.0132, Train Acc: 1.0000, Val Loss: 0.0587, Val Acc: 0.9556\n",
      "Epoch 740/1000, Train Loss: 0.0130, Train Acc: 1.0000, Val Loss: 0.0584, Val Acc: 0.9778\n",
      "Epoch 750/1000, Train Loss: 0.0129, Train Acc: 1.0000, Val Loss: 0.0580, Val Acc: 0.9778\n",
      "Epoch 760/1000, Train Loss: 0.0127, Train Acc: 1.0000, Val Loss: 0.0577, Val Acc: 0.9778\n",
      "Epoch 770/1000, Train Loss: 0.0126, Train Acc: 1.0000, Val Loss: 0.0574, Val Acc: 0.9778\n",
      "Epoch 780/1000, Train Loss: 0.0124, Train Acc: 1.0000, Val Loss: 0.0571, Val Acc: 0.9778\n",
      "Epoch 790/1000, Train Loss: 0.0123, Train Acc: 1.0000, Val Loss: 0.0568, Val Acc: 0.9778\n",
      "Epoch 800/1000, Train Loss: 0.0121, Train Acc: 1.0000, Val Loss: 0.0565, Val Acc: 0.9778\n",
      "Epoch 810/1000, Train Loss: 0.0120, Train Acc: 1.0000, Val Loss: 0.0562, Val Acc: 0.9778\n",
      "Epoch 820/1000, Train Loss: 0.0118, Train Acc: 1.0000, Val Loss: 0.0559, Val Acc: 0.9778\n",
      "Epoch 830/1000, Train Loss: 0.0117, Train Acc: 1.0000, Val Loss: 0.0557, Val Acc: 0.9778\n",
      "Epoch 840/1000, Train Loss: 0.0116, Train Acc: 1.0000, Val Loss: 0.0554, Val Acc: 0.9778\n",
      "Epoch 850/1000, Train Loss: 0.0115, Train Acc: 1.0000, Val Loss: 0.0551, Val Acc: 0.9778\n",
      "Epoch 860/1000, Train Loss: 0.0113, Train Acc: 1.0000, Val Loss: 0.0549, Val Acc: 0.9778\n",
      "Epoch 870/1000, Train Loss: 0.0112, Train Acc: 1.0000, Val Loss: 0.0546, Val Acc: 0.9778\n",
      "Epoch 880/1000, Train Loss: 0.0111, Train Acc: 1.0000, Val Loss: 0.0544, Val Acc: 0.9778\n",
      "Epoch 890/1000, Train Loss: 0.0110, Train Acc: 1.0000, Val Loss: 0.0541, Val Acc: 0.9778\n",
      "Epoch 900/1000, Train Loss: 0.0109, Train Acc: 1.0000, Val Loss: 0.0539, Val Acc: 0.9778\n",
      "Epoch 910/1000, Train Loss: 0.0108, Train Acc: 1.0000, Val Loss: 0.0537, Val Acc: 0.9778\n",
      "Epoch 920/1000, Train Loss: 0.0107, Train Acc: 1.0000, Val Loss: 0.0535, Val Acc: 0.9778\n",
      "Epoch 930/1000, Train Loss: 0.0106, Train Acc: 1.0000, Val Loss: 0.0533, Val Acc: 0.9778\n",
      "Epoch 940/1000, Train Loss: 0.0105, Train Acc: 1.0000, Val Loss: 0.0531, Val Acc: 0.9778\n",
      "Epoch 950/1000, Train Loss: 0.0104, Train Acc: 1.0000, Val Loss: 0.0529, Val Acc: 0.9778\n",
      "Epoch 960/1000, Train Loss: 0.0103, Train Acc: 1.0000, Val Loss: 0.0527, Val Acc: 0.9778\n",
      "Epoch 970/1000, Train Loss: 0.0102, Train Acc: 1.0000, Val Loss: 0.0525, Val Acc: 0.9778\n",
      "Epoch 980/1000, Train Loss: 0.0101, Train Acc: 1.0000, Val Loss: 0.0523, Val Acc: 0.9778\n",
      "Epoch 990/1000, Train Loss: 0.0100, Train Acc: 1.0000, Val Loss: 0.0522, Val Acc: 0.9778\n",
      "Epoch 1000/1000, Train Loss: 0.0099, Train Acc: 1.0000, Val Loss: 0.0520, Val Acc: 0.9778\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = nn.fit(x_train, y_train, x_val, y_val, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c3ccd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a1e0603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear1 parameters: 300\n",
      "Linear2 parameters: 20200\n",
      "Linear3 parameters: 20100\n",
      "Linear4 parameters: 303\n",
      "total parameters: 40903\n",
      "Model loaded from best-nonlinear-model.npz\n",
      "Test Accuracy: 0.9333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       1.00      0.93      0.97        15\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.94      0.93      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14  0  1]\n",
      " [ 1 14  0]\n",
      " [ 1  0 14]]\n"
     ]
    }
   ],
   "source": [
    "trained_model = ArtificialNeuralNetwork(2, 3)\n",
    "trained_model.load(\"best-nonlinear-model.npz\")\n",
    "\n",
    "y_pred_test = trained_model.predict(x_test)\n",
    "test_acc = np.mean(y_pred_test == y_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
